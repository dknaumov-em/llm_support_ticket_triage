{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69345a2a-7d4c-4501-9e65-d337cc3f1ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DistributedSampler\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "import evaluate\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "from LLMClasses import AdapterLlamaMLP, LoRALinear, LLMmanager\n",
    "from GeneratorDataClasses import EvaluationDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb597d-902e-41c0-be09-80e7ff1e0734",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e770b277-ba3d-47d1-b559-8c6ca59d1fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "EVALUATION_STAT_FILENAME = 'original_evaluation_stats.json'\n",
    "EVALUATION_DF_FILENAME = 'original_evaluation_df.json'\n",
    "\n",
    "TRAIN_FILENAME = 'llm_processed_tickets_df_train.json'\n",
    "VAL_FILENAME = 'llm_processed_tickets_df_val.json'\n",
    "\n",
    "CONTEXT_FILENAME = 'embedded_context_df.json'\n",
    "SUMMARY_FILENAME = 'llm_summarized_tickets_df.json'\n",
    "DATA_DIRECTORY = Path.cwd().joinpath('Data')\n",
    "\n",
    "context_embedding_cols = ['Query embedding', 'Answer embedding', 'context_sims']\n",
    "contex_dtypes = {'Query' : str, 'Answer' : str,\n",
    "                 'Query embedding' : list, 'Answer embedding' : list,\n",
    "                 'context_ids' : list, 'context_sims' : list}\n",
    "\n",
    "# Import context dataset\n",
    "\n",
    "context_tickets_df = pd.read_json(DATA_DIRECTORY.joinpath(CONTEXT_FILENAME), \n",
    "                                  orient = 'index', typ = 'frame', \n",
    "                                  dtype = contex_dtypes, precise_float = True)\n",
    "\n",
    "for col in context_embedding_cols:\n",
    "    context_tickets_df[col] = context_tickets_df[col].apply(lambda x: np.array(x, dtype = np.float32))\n",
    "\n",
    "# Import SFT dataset\n",
    "\n",
    "processed_tickets_df_train = pd.read_json(DATA_DIRECTORY.joinpath(TRAIN_FILENAME), \n",
    "                                            orient = 'index', typ = 'frame', \n",
    "                                            dtype = str, precise_float = True)\n",
    "\n",
    "processed_tickets_df_val = pd.read_json(DATA_DIRECTORY.joinpath(VAL_FILENAME), \n",
    "                                            orient = 'index', typ = 'frame', \n",
    "                                            dtype = str, precise_float = True)\n",
    "\n",
    "# Import and integrate ticket history summary dataset\n",
    "\n",
    "summarized_tickets_df = pd.read_json(DATA_DIRECTORY.joinpath(SUMMARY_FILENAME), \n",
    "                                     orient = 'index', typ = 'frame', \n",
    "                                     dtype = str, precise_float = True)\n",
    "\n",
    "summarized_tickets_df.drop(columns = ['PROBLEM', 'SOLUTION', 'STRUCTUREDPROBLEM', 'STRUCTUREDSOLUTION'], inplace = True)\n",
    "summarized_tickets_df.set_index('TICKETID', drop = True, inplace = True)\n",
    "\n",
    "context_tickets_df = context_tickets_df.join(summarized_tickets_df)\n",
    "context_tickets_df['Summarized Answer'] = \\\n",
    "    context_tickets_df['Answer'].str.split('\\n\\nTicket status history:\\n\\n').str[0] \\\n",
    "        + '\\n\\nActivities description:\\n\\n' \\\n",
    "        + context_tickets_df['SUMMARIZEDSOLUTION']\n",
    "\n",
    "context_tickets_df.sort_index(inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aed847-6122-4b21-bb95-70c234acdc1c",
   "metadata": {},
   "source": [
    "## Model Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c04e99e-67fb-41fe-8a43-0ac958fb31d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configuration and import parameters\n",
    "\n",
    "PEFT = False\n",
    "FROM_PRETRAINED = False\n",
    "IS_DUMMY = True\n",
    "DUMMY_PARAMETERS = {'hidden_size' : 2, 'intermediate_size' : 4, 'head_dim' : 8}\n",
    "LOAD_8BIT = False\n",
    "LOAD_FINETUNED = False\n",
    "PARALLELIZE = True\n",
    "USE_CACHE = True\n",
    "\n",
    "USE_GPU = True\n",
    "DEVICE = torch.device('mps' if torch.mps.is_available() and USE_GPU else ('cuda' if torch.cuda.is_available() and USE_GPU else 'cpu'))\n",
    "\n",
    "MODEL_DICT = \\\n",
    "    {'name' : 'Llama-3-8B-Instruct',\n",
    "     'repo_id' : 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "     'required_files' : ['config.json', 'generation_config.json', 'model.safetensors',\n",
    "                         'model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors',\n",
    "                         'model-00003-of-00004.safetensors', 'model-00004-of-00004.safetensors',\n",
    "                         'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'model.safetensors.index.json'],\n",
    "     'model_path' : ['LLaMa', '3.1-8B-Instruct']}\n",
    "\n",
    "ROPE_SCALING = None\n",
    "MAX_CONTEXT_LENGTH = 16000\n",
    "ROPE_THETA = 1000000\n",
    "MAX_GENERATION_LENGTH = 1000\n",
    "\n",
    "# Data options\n",
    "\n",
    "DATASET_TYPE = 'SFT'\n",
    "NUM_RETRIEVED = 5\n",
    "INCLUDES_GOLD_PROB = 0.6\n",
    "RELEVANT_PROB_DIST = [0.5, 0.2, 0.1, 0.1, 0.1]\n",
    "IRRELEVANT_PROB_DIST = [0.7, 0.1, 0.05, 0.05, 0.05, 0.05]\n",
    "\n",
    "# Model import\n",
    "\n",
    "LLM_PATH = Path.cwd().joinpath(*MODEL_DICT['model_path'])\n",
    "FINETUNED_MODEL_PATH = LLM_PATH.joinpath('SFT', f\"Checkpoint_{MODEL_DICT['name']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9715fd-a1b4-4391-8bf6-494fece8f335",
   "metadata": {},
   "source": [
    "## Post-training validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb949579-14f7-40ec-9942-e615276732f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and  prepare the LLM for fine-tuning\n",
    "\n",
    "llm_utility = \\\n",
    "    LLMmanager(LLM_PATH, DEVICE, finetuned_model_path = FINETUNED_MODEL_PATH, use_cache = USE_CACHE,\n",
    "               peft = PEFT, from_pretrained = FROM_PRETRAINED, load_8bit = LOAD_8BIT,\n",
    "               is_dummy = IS_DUMMY, dummy_parameters = DUMMY_PARAMETERS,\n",
    "               rope_scaling = ROPE_SCALING, rope_theta = ROPE_THETA, max_context_length = MAX_CONTEXT_LENGTH)\n",
    "\n",
    "tokenizer, llm_model, gen_config = llm_utility.load_model(load_finetuned = LOAD_FINETUNED, parallelize = PARALLELIZE)\n",
    "\n",
    "with open(LLM_PATH.joinpath('system_prompt.txt')) as sp, \\\n",
    "    open(LLM_PATH.joinpath('generation_template.txt')) as gt:\n",
    "        \n",
    "    system_prompt = sp.read()\n",
    "    generation_template = gt.read()\n",
    "\n",
    "tokenizer.chat_template = generation_template\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# Instantiate a dataset\n",
    "\n",
    "dataset_train = EvaluationDataset(tokenizer, processed_tickets_df_train, DATASET_TYPE,\n",
    "                                target_col = 'SOLUTION', system_prompt = system_prompt, query_col = 'PROBLEM',\n",
    "                                context_dataset = context_tickets_df, query_con_col = 'Query', id_col = 'TICKETID',\n",
    "                                answer_con_col = 'Summarized Answer', query_match_col = 'context_ids', sim_match_col = 'context_sims',\n",
    "                                num_retrieved = NUM_RETRIEVED, includes_gold_prob = INCLUDES_GOLD_PROB,\n",
    "                                relevant_prob_dist = RELEVANT_PROB_DIST, irrelevant_prob_dist = IRRELEVANT_PROB_DIST,\n",
    "                                max_context_length = MAX_CONTEXT_LENGTH, max_generation_length = MAX_GENERATION_LENGTH, train = True)\n",
    "\n",
    "\n",
    "if VAL_FILENAME is not None:\n",
    "\n",
    "    dataset_val = EvaluationDataset(tokenizer, processed_tickets_df_val, DATASET_TYPE,\n",
    "                                  target_col = 'SOLUTION', system_prompt = system_prompt, query_col = 'PROBLEM',\n",
    "                                  context_dataset = context_tickets_df, query_con_col = 'Query', id_col = 'TICKETID',\n",
    "                                  answer_con_col = 'Summarized Answer', query_match_col = 'context_ids', sim_match_col = 'context_sims',\n",
    "                                  num_retrieved = NUM_RETRIEVED, includes_gold_prob = INCLUDES_GOLD_PROB,\n",
    "                                  relevant_prob_dist = RELEVANT_PROB_DIST, irrelevant_prob_dist = IRRELEVANT_PROB_DIST,\n",
    "                                  max_context_length = MAX_CONTEXT_LENGTH, max_generation_length = MAX_GENERATION_LENGTH, train = False)\n",
    "\n",
    "else:\n",
    "\n",
    "    dataset_val = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "85292956-264a-4b1b-99f3-e69fb9b284dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post training evaluation functions\n",
    "\n",
    "def distinct_n_corpus(corpus, n):\n",
    "\n",
    "    all_ngrams = []\n",
    "    total_ngrams = 0\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        tokens = sentence.split()\n",
    "        total_ngrams += max(len(tokens) - n + 1, 0)\n",
    "        all_ngrams.extend(list(zip(*[tokens[i:] for i in range(n)])))\n",
    "        \n",
    "    if total_ngrams == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    return len(set(all_ngrams)) / total_ngrams\n",
    "\n",
    "def evaluate_text_generation(model, tokenizer, dataset_train, gen_config, dataset_val = None, batch_size = 4):\n",
    "\n",
    "    setattr(gen_config, 'do_sample', False)\n",
    "    \n",
    "    ref_nlls = []\n",
    "    preds, refs, is_train_list, input_list = [], [], [], []\n",
    "    includes_gold_list, number_of_random_examples_list = [], []\n",
    "    gen_tokens_length_list, prompt_tokens_length_list = [], []\n",
    "\n",
    "    if dataset_val is None:\n",
    "        dataloader = DataLoader(dataset_train, batch_size = batch_size, shuffle = True, collate_fn = dataset_train.batch_collate)\n",
    "    else:\n",
    "        dataloader = DataLoader(ConcatDataset([dataset_train, dataset_val]), batch_size = batch_size,\n",
    "                                    shuffle = True, collate_fn = dataset_train.batch_collate)\n",
    "\n",
    "    bleu_metric = evaluate.load('bleu')\n",
    "    rouge_metric = evaluate.load('rouge')\n",
    "    meteor_metric = evaluate.load('meteor')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for input_ids, attention_mask, labels, input_texts, target_texts, is_train, includes_gold, random_num in tqdm(dataloader):\n",
    "                \n",
    "            input_ids = input_ids.to(model.device)\n",
    "            labels = labels.to(model.device)\n",
    "    \n",
    "            logits = model(input_ids = input_ids, attention_mask = attention_mask).logits\n",
    "            shifted_logits = logits[:, :-1, :].contiguous()\n",
    "            shifted_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "            vocab_size = shifted_logits.size(-1)\n",
    "            \n",
    "            losses_flat = nn.functional.cross_entropy(\n",
    "                shifted_logits.view(-1, vocab_size),\n",
    "                shifted_labels.view(-1),\n",
    "                reduction = 'none',\n",
    "                ignore_index = -100)\n",
    "\n",
    "            token_losses = losses_flat.view(input_ids.size(0), -1)\n",
    "            mask = (shifted_labels != -100).float()\n",
    "            tokens_per_example = mask.sum(dim = 1).clamp(min = 1.0)\n",
    "\n",
    "            nll_per_example = (token_losses * mask).sum(dim = 1) / tokens_per_example\n",
    "            ref_nlls.extend(nll_per_example.cpu().tolist())\n",
    "\n",
    "            # Generate predictions\n",
    "\n",
    "            inputs = tokenizer(input_texts, return_tensors = 'pt', padding = True,\n",
    "                                   padding_side = 'left').to(orig_model.device)\n",
    "\n",
    "            gen_tokens = model.generate(inputs['input_ids'], attention_mask = inputs['attention_mask'],\n",
    "                                            pad_token_id = tokenizer.pad_token_id, generation_config = gen_config,\n",
    "                                               return_dict_in_generate = True, output_scores = False)\n",
    "\n",
    "            prompt_lengths = inputs['attention_mask'].sum(dim = 1).long().cpu().tolist()\n",
    "\n",
    "            sequences = gen_tokens.sequences \n",
    "            gen_tokens_list = []\n",
    "            \n",
    "            for i in range(sequences.size(0)):\n",
    "                \n",
    "                seq = sequences[i]\n",
    "                p_len = prompt_lengths[i]\n",
    "                gen = seq[inputs['input_ids'].size(1):]\n",
    "                gen_list = gen.cpu().tolist()\n",
    "                gen_len = 0\n",
    "                \n",
    "                for token in gen_list:\n",
    "                    if token == tokenizer.eos_token_id or token == tokenizer.pad_token_id:\n",
    "                        break\n",
    "                    gen_len += 1\n",
    "\n",
    "                prompt_tokens_length_list.append(p_len)\n",
    "                gen_tokens_length_list.append(gen_len)\n",
    "                    \n",
    "                if gen_len == 0:\n",
    "                    gen_tokens_list.append(torch.tensor([], dtype = torch.long))\n",
    "                else:\n",
    "                    gen_tokens_list.append(gen[:gen_len].cpu())\n",
    "\n",
    "            pred_texts = [tokenizer.decode(gen.cpu().tolist(), skip_special_tokens = True) for gen in gen_tokens_list]\n",
    "\n",
    "            input_list.extend(input_texts)\n",
    "            preds.extend(pred_texts)\n",
    "            refs.extend(target_texts)\n",
    "            is_train_list.extend(is_train)\n",
    "            includes_gold_list.extend(includes_gold)\n",
    "            number_of_random_examples_list.extend(random_num)\n",
    "\n",
    "    # BLEU, ROUGE, METEOR\n",
    "\n",
    "    bleu_scores = []\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    meteor_scores = []\n",
    "    \n",
    "    for pred, ref in zip(preds, refs):\n",
    "        \n",
    "        bleu = bleu_metric.compute(predictions = [pred], references = [[ref]])['bleu']\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            rouge = rouge_metric.compute(predictions = [pred], references = [ref], use_aggregator = True)\n",
    "            rouge1 = rouge.get('rouge1', 0.0)\n",
    "            rouge2 = rouge.get('rouge2', 0.0) \n",
    "            rougeL = rouge.get('rougeL', 0.0)   \n",
    "            \n",
    "        meteor = meteor_metric.compute(predictions = [pred], references = [ref])['meteor']\n",
    "        \n",
    "        bleu_scores.append(bleu)\n",
    "        rouge1_scores.append(rouge1)\n",
    "        rouge2_scores.append(rouge2)\n",
    "        rougeL_scores.append(rougeL)\n",
    "        meteor_scores.append(meteor)\n",
    "\n",
    "    # BERTScore\n",
    "    \n",
    "    P, R, F1 = bert_score(preds, refs, lang = 'en', model_type = 'roberta-large',\n",
    "                              rescale_with_baseline = True)\n",
    "\n",
    "    bert_P = P.cpu().tolist()\n",
    "    bert_R = R.cpu().tolist()\n",
    "    bert_F1 = F1.cpu().tolist()\n",
    "\n",
    "    # Diversity\n",
    "    \n",
    "    distinct1 = distinct_n_corpus([preds[i] for i in range(len(preds))], 1)\n",
    "    distinct2 = distinct_n_corpus([preds[i] for i in range(len(preds))], 2)\n",
    "\n",
    "    distinct1_gold = distinct_n_corpus([preds[i] for i in range(len(preds)) if includes_gold_list[i]], 1)\n",
    "    distinct2_gold = distinct_n_corpus([preds[i] for i in range(len(preds)) if includes_gold_list[i]], 2)\n",
    "\n",
    "    distinct1_related = distinct_n_corpus([preds[i] for i in range(len(preds)) if not includes_gold_list[i]], 1)\n",
    "    distinct2_related = distinct_n_corpus([preds[i] for i in range(len(preds)) if not includes_gold_list[i]], 2)\n",
    "\n",
    "    if dataset_val is not None:\n",
    "\n",
    "        distinct1_train = distinct_n_corpus([preds[i] for i in range(len(preds)) if is_train_list[i]], 1)\n",
    "        distinct2_train = distinct_n_corpus([preds[i] for i in range(len(preds)) if is_train_list[i]], 2)\n",
    "\n",
    "        distinct1_val = distinct_n_corpus([preds[i] for i in range(len(preds)) if not is_train_list[i]], 1)\n",
    "        distinct2_val = distinct_n_corpus([preds[i] for i in range(len(preds)) if not is_train_list[i]], 2)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'input_text': input_list,\n",
    "        'prediction': preds,\n",
    "        'target_text': refs,\n",
    "        'is_train': is_train_list,\n",
    "        'includes_gold': includes_gold_list,\n",
    "        'n_random_examples': number_of_random_examples_list,\n",
    "        'prompth_length' : prompt_tokens_length_list,\n",
    "        'gen_length' : gen_tokens_length_list,\n",
    "        'CEloss': ref_nlls,\n",
    "        'Perplexity' : list(np.exp(np.clip(np.array(ref_nlls), -100, 100))),\n",
    "        'BLEU': bleu_scores,\n",
    "        'Rouge1': rouge1_scores,\n",
    "        'Rouge2': rouge2_scores,\n",
    "        'RougeL': rougeL_scores,\n",
    "        'METEOR': meteor_scores,\n",
    "        'bertscore_precision': bert_P,\n",
    "        'bertscore_recall': bert_R,\n",
    "        'bertscore_f1': bert_F1,\n",
    "    })\n",
    "\n",
    "    stat_list = ['CEloss', 'Perplexity', 'BLEU', 'Rouge1', 'Rouge2', 'RougeL', 'METEOR',\n",
    "                 'bertscore_precision', 'bertscore_recall', 'bertscore_f1']\n",
    "    stat_dict = {}\n",
    "\n",
    "    for col in stat_list:\n",
    "\n",
    "        if dataset_val is not None:\n",
    "            stat_dict[col] = {\n",
    "                'Train' : df.loc[np.array(is_train_list), col].mean(),\n",
    "                'Val' : df.loc[~np.array(is_train_list), col].mean(),\n",
    "                'Gold' : df.loc[np.array(includes_gold_list), col].mean(),\n",
    "                'Related' : df.loc[~np.array(includes_gold_list), col].mean(),\n",
    "                'Overall' : df.loc[:, col].mean()\n",
    "            }\n",
    "        else:\n",
    "            stat_dict[col] = {'Overall' : df.loc[:, col].mean(),}\n",
    "\n",
    "    if dataset_val is not None:\n",
    "        \n",
    "        stat_dict['Distinct1'] = {\n",
    "                    'Train' : distinct1_train,\n",
    "                    'Val' : distinct1_val,\n",
    "                    'Gold' : distinct1_gold,\n",
    "                    'Related' : distinct1_related,\n",
    "                    'Overall' : distinct1\n",
    "        }\n",
    "\n",
    "        stat_dict['Distinct2'] = {\n",
    "            'Train' : distinct2_train,\n",
    "            'Val' : distinct2_val,\n",
    "            'Gold' : distinct2_gold,\n",
    "            'Related' : distinct2_related,\n",
    "            'Overall' : distinct2\n",
    "            \n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        stat_dict['Distinct1'] = {'Overall' : distinct1,\n",
    "                                  'Gold' : distinct1_gold,\n",
    "                                  'Related' : distinct1_related}\n",
    "        stat_dict['Distinct2'] = {'Overall' : distinct2,\n",
    "                                  'Gold' : distinct2_gold,\n",
    "                                  'Related' : distinct2_related}\n",
    "    \n",
    "    \n",
    "    return stat_dict, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "03a3fb35-2ea1-4ff2-8d95-6e64a16f6770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute evaluation function and save results\n",
    "\n",
    "evaluation_stats, evaluation_df = evaluate_text_generation(llm_model, tokenizer, dataset_train, gen_config,\n",
    "                                                           dataset_val = dataset_val, batch_size = BATCH_SIZE)\n",
    "\n",
    "with open(DATA_DIRECTORY.joinpath(EVALUATION_STAT_FILENAME), 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(evaluation_stats, f, ensure_ascii = False, indent = True)\n",
    "\n",
    "evaluation_df.to_json(DATA_DIRECTORY.joinpath(EVALUATION_DF_FILENAME),\n",
    "                                 orient = 'index', double_precision = 15, index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22483fbf-9955-43ba-b7ad-3711d3aeeaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_utility = \\\n",
    "    LLMmanager(LLM_PATH, DEVICE, finetuned_model_path = FINETUNED_MODEL_PATH, use_cache = USE_CACHE,\n",
    "               peft = PEFT, from_pretrained = FROM_PRETRAINED, load_8bit = LOAD_8BIT,\n",
    "               is_dummy = IS_DUMMY, dummy_parameters = DUMMY_PARAMETERS,\n",
    "               rope_scaling = ROPE_SCALING, rope_theta = ROPE_THETA, max_context_length = MAX_CONTEXT_LENGTH)\n",
    "\n",
    "tokenizer, llm_model, gen_config = llm_utility.load_model(load_finetuned = LOAD_FINETUNED, parallelize = PARALLELIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877a432f-e044-4394-b406-b00b0430a3cc",
   "metadata": {},
   "source": [
    "## LLL-as-Judge validation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394021b-8daa-4e92-a76e-3a76686b1441",
   "metadata": {},
   "outputs": [],
   "source": [
    "or_judged_df = pd.read_json('Data/or_llm_judged_tickets.json', \n",
    "                            orient = 'index', typ = 'frame', \n",
    "                            dtype = str, precise_float = True)\n",
    "sft_judged_df = pd.read_json('Data/sft_llm_judged_tickets.json', \n",
    "                             orient = 'index', typ = 'frame', \n",
    "                             dtype = str, precise_float = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb66c2e-7b01-4b0f-8b84-88948a7e21ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "or_judged_df['LLMJudgement'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a1dd2b-7d05-4864-9608-cc948b6bb0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_judged_df['LLMJudgement'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd81ee-765b-43e5-bb15-66a54a2321e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_judged_df.assign(is_helpful = sft_judged_df['LLMJudgement'].isin(['FULL_MATCH', 'PARTIAL_MATCH_HELPFUL'])) \\\n",
    "    .groupby('includes_gold', as_index = True)['is_helpful'].sum() \\\n",
    "        / sft_judged_df.query(\"LLMJudgement != 'None'\").groupby('includes_gold', as_index = True)['LLMJudgement'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e19c05-8939-47b2-8c56-f7c080062f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_judged_df.assign(is_helpful = sft_judged_df['LLMJudgement'].isin(['FULL_MATCH', 'PARTIAL_MATCH_HELPFUL'])) \\\n",
    "    .groupby(['includes_gold', 'is_train'], as_index = True)['is_helpful'].sum() \\\n",
    "        / sft_judged_df.query(\"LLMJudgement != 'None'\").groupby(['includes_gold', 'is_train'], as_index = True)['LLMJudgement'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb68fd-cd2c-4c2f-ba34-279d7a1dcdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_judged_df.assign(is_helpful = sft_judged_df['LLMJudgement'].isin(['FULL_MATCH', 'PARTIAL_MATCH_HELPFUL'])) \\\n",
    "    .groupby(['includes_gold', 'n_random_examples'], as_index = True)['is_helpful'].sum() \\\n",
    "        / sft_judged_df.query(\"LLMJudgement != 'None'\") \\\n",
    "              .groupby(['includes_gold', 'n_random_examples'], as_index = True)['LLMJudgement'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc71db7-0d81-411a-8764-2a8fdfb74b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "or_judged_df.assign(is_helpful = or_judged_df['LLMJudgement'].isin(['FULL_MATCH', 'PARTIAL_MATCH_HELPFUL'])) \\\n",
    "    .groupby('includes_gold', as_index = True)['is_helpful'].sum() \\\n",
    "        / or_judged_df.query(\"LLMJudgement != 'None'\").groupby('includes_gold', as_index = True)['LLMJudgement'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffbdd0f-c087-4386-93f0-429e68c0cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "or_judged_df.assign(is_helpful = or_judged_df['LLMJudgement'].isin(['FULL_MATCH', 'PARTIAL_MATCH_HELPFUL'])) \\\n",
    "    .groupby(['includes_gold', 'is_train'], as_index = True)['is_helpful'].sum() \\\n",
    "        / or_judged_df.query(\"LLMJudgement != 'None'\").groupby(['includes_gold', 'is_train'], as_index = True)['LLMJudgement'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07816127-3450-43ba-9a31-c053e1e35e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "or_judged_df.assign(is_helpful = or_judged_df['LLMJudgement'].isin(['FULL_MATCH', 'PARTIAL_MATCH_HELPFUL'])) \\\n",
    "    .groupby(['includes_gold', 'n_random_examples'], as_index = True)['is_helpful'].sum() \\\n",
    "        / or_judged_df.query(\"LLMJudgement != 'None'\") \\\n",
    "              .groupby(['includes_gold', 'n_random_examples'], as_index = True)['LLMJudgement'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662830e6-da35-4638-ab18-52a8c03c0d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69345a2a-7d4c-4501-9e65-d337cc3f1ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DistributedSampler\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from huggingface_hub import login\n",
    "from safetensors.torch import load_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb597d-902e-41c0-be09-80e7ff1e0734",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "e770b277-ba3d-47d1-b559-8c6ca59d1fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TICKETID</th>\n",
       "      <th>PROBLEM</th>\n",
       "      <th>SOLUTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t6UJ9A00EVYR</td>\n",
       "      <td>Ticket metadata:\\n\\n  Ticket ID: t6UJ9A00EVYR\\...</td>\n",
       "      <td>[Urgency: 1]\\n\\n**Issue Summary**  \\nThe user ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t6UJ9A00EVZA</td>\n",
       "      <td>Ticket metadata:\\n\\n  Ticket ID: t6UJ9A00EVZA\\...</td>\n",
       "      <td>[Urgency: 1]\\n\\n**Issue Summary**  \\nA daily c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t6UJ9A00EW08</td>\n",
       "      <td>Ticket metadata:\\n\\n  Ticket ID: t6UJ9A00EW08\\...</td>\n",
       "      <td>[Urgency: 3]\\n\\n**Issue Summary**  \\nA user is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t6UJ9A00EW0A</td>\n",
       "      <td>Ticket metadata:\\n\\n  Ticket ID: t6UJ9A00EW0A\\...</td>\n",
       "      <td>[Urgency: 3]\\n\\n**Issue Summary**  \\nThe custo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t6UJ9A00EW0K</td>\n",
       "      <td>Ticket metadata:\\n\\n  Ticket ID: t6UJ9A00EW0K\\...</td>\n",
       "      <td>[Urgency: 3]\\n\\n**Issue Summary**  \\nA user is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TICKETID                                            PROBLEM  \\\n",
       "0  t6UJ9A00EVYR  Ticket metadata:\\n\\n  Ticket ID: t6UJ9A00EVYR\\...   \n",
       "1  t6UJ9A00EVZA  Ticket metadata:\\n\\n  Ticket ID: t6UJ9A00EVZA\\...   \n",
       "2  t6UJ9A00EW08  Ticket metadata:\\n\\n  Ticket ID: t6UJ9A00EW08\\...   \n",
       "3  t6UJ9A00EW0A  Ticket metadata:\\n\\n  Ticket ID: t6UJ9A00EW0A\\...   \n",
       "4  t6UJ9A00EW0K  Ticket metadata:\\n\\n  Ticket ID: t6UJ9A00EW0K\\...   \n",
       "\n",
       "                                            SOLUTION  \n",
       "0  [Urgency: 1]\\n\\n**Issue Summary**  \\nThe user ...  \n",
       "1  [Urgency: 1]\\n\\n**Issue Summary**  \\nA daily c...  \n",
       "2  [Urgency: 3]\\n\\n**Issue Summary**  \\nA user is...  \n",
       "3  [Urgency: 3]\\n\\n**Issue Summary**  \\nThe custo...  \n",
       "4  [Urgency: 3]\\n\\n**Issue Summary**  \\nA user is...  "
      ]
     },
     "execution_count": 726,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data import\n",
    "\n",
    "# Data parameters\n",
    "\n",
    "sft_filename = 'llm_processed_tickets_df.json'\n",
    "cpt_filename = 'sage_docs.json'\n",
    "context_filename = 'embedded_context_df.json'\n",
    "summary_filename = 'llm_summarized_tickets_df.json'\n",
    "data_folder = Path.cwd().joinpath('Data')\n",
    "\n",
    "context_embedding_cols = ['Query embedding', 'Answer embedding', 'context_sims']\n",
    "contex_dtypes = {'Query' : str, 'Answer' : str,\n",
    "                 'Query embedding' : list, 'Answer embedding' : list,\n",
    "                 'context_ids' : list, 'context_sims' : list}\n",
    "\n",
    "# Import sage documents\n",
    "\n",
    "with open(data_folder.joinpath(cpt_filename)) as f:\n",
    "    sage_docs = json.load(f)\n",
    "    \n",
    "sage_docs = \\\n",
    "    [doc['info'] + '\\n\\n'\n",
    "         + doc['metadata'].get('Keywords', '') + ('\\n\\n' if doc['metadata'].get('Keywords', '') else '')\n",
    "         + doc['metadata'].get('Product', '') + ('\\n\\n' if doc['metadata'].get('Product', '') else '')\n",
    "         + doc['title'] + '\\n\\n' \n",
    "         + doc['content'] \n",
    "             for doc in sage_docs]\n",
    "\n",
    "# Import context dataset\n",
    "\n",
    "context_tickets_df = pd.read_json(data_folder.joinpath(context_filename), \n",
    "                                  orient = 'index', typ = 'frame', \n",
    "                                  dtype = contex_dtypes, precise_float = True)\n",
    "\n",
    "for col in context_embedding_cols:\n",
    "    context_tickets_df[col] = context_tickets_df[col].apply(lambda x: np.array(x, dtype = np.float32))\n",
    "\n",
    "# Import SFT dataset\n",
    "\n",
    "processed_tickets_df = pd.read_json(data_folder.joinpath(sft_filename), \n",
    "                                    orient = 'index', typ = 'frame', \n",
    "                                    dtype = str, precise_float = True)\n",
    "\n",
    "processed_tickets_df.drop(columns = ['PROBLEM', 'SOLUTION', 'STRUCTUREDSOLUTION'], inplace = True)\n",
    "processed_tickets_df.rename(columns = {'STRUCTUREDPROBLEM' : 'PROBLEM', 'SUMMARIZEDSOLUTION' : 'SOLUTION'}, inplace = True)\n",
    "\n",
    "# Import and integrate ticket history summary dataset\n",
    "\n",
    "summarized_tickets_df = pd.read_json(data_folder.joinpath(summary_filename), \n",
    "                                     orient = 'index', typ = 'frame', \n",
    "                                     dtype = str, precise_float = True)\n",
    "\n",
    "summarized_tickets_df.drop(columns = ['PROBLEM', 'SOLUTION', 'STRUCTUREDPROBLEM', 'STRUCTUREDSOLUTION'], inplace = True)\n",
    "summarized_tickets_df.set_index('TICKETID', drop = True, inplace = True)\n",
    "\n",
    "context_tickets_df = context_tickets_df.join(summarized_tickets_df)\n",
    "context_tickets_df['Summarized Answer'] = \\\n",
    "    context_tickets_df['Answer'].str.split('\\n\\nTicket status history:\\n\\n').str[0] \\\n",
    "        + '\\n\\nActivities description:\\n\\n' \\\n",
    "        + context_tickets_df['SUMMARIZEDSOLUTION']\n",
    "\n",
    "# Sort both datasets\n",
    "\n",
    "context_tickets_df.sort_index(inplace = True)\n",
    "processed_tickets_df.sort_values('TICKETID', ignore_index = True, inplace = True)\n",
    "\n",
    "# Display the dataset\n",
    "\n",
    "processed_tickets_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aed847-6122-4b21-bb95-70c234acdc1c",
   "metadata": {},
   "source": [
    "## Model Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "id": "6c04e99e-67fb-41fe-8a43-0ac958fb31d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8fd9a8ff66417e9708ddaddbc99828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define model configuration and import parameters\n",
    "\n",
    "PEFT = True\n",
    "FROM_PRETRAINED = False\n",
    "IS_DUMMY = True\n",
    "DUMMY_PARAMETERS = {'hidden_size' : 2, 'intermediate_size' : 4, 'head_dim' : 8}\n",
    "LOAD_8BIT = False\n",
    "LOAD_FINETUNED = True\n",
    "PARALLELIZE = False\n",
    "USE_CACHE = False\n",
    "\n",
    "USE_GPU = True\n",
    "DEVICE = torch.device('mps' if torch.mps.is_available() and USE_GPU else ('cuda' if torch.cuda.is_available() and USE_GPU else 'cpu'))\n",
    "\n",
    "MODEL_DICT = \\\n",
    "    {'name' : 'Llama-3-8B-Instruct',\n",
    "     'repo_id' : 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "     'required_files' : ['config.json', 'generation_config.json', 'model.safetensors',\n",
    "                         'model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors',\n",
    "                         'model-00003-of-00004.safetensors', 'model-00004-of-00004.safetensors',\n",
    "                         'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'model.safetensors.index.json'],\n",
    "     'model_path' : ['LLaMa', '3.1-8B-Instruct']}\n",
    "\n",
    "ROPE_SCALING = None\n",
    "MAX_CONTEXT_LENGTH = 16000\n",
    "ROPE_THETA = 1000000\n",
    "MAX_GENERATION_LENGTH = 10000\n",
    "\n",
    "ADAPTER_DIM = 128\n",
    "LORA_RANK = 16\n",
    "LORA_ALPHA = 16\n",
    "\n",
    "LORA_PARAMS = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
    "ORIGINAL_TRAINABLE_PARAMS = ['input_layernorm', 'post_attention_layernorm', 'norm', 'lm_head']\n",
    "TRAINABLE_ADAPTER = True\n",
    "TRAINABLE_LORA = True\n",
    "\n",
    "# Data options\n",
    "\n",
    "VALIDATION = True\n",
    "DATASET_TYPE = 'SFT'\n",
    "NUM_RETRIEVED = 5\n",
    "INCLUDES_GOLD_PROB = 0.7\n",
    "RELEVANT_PROB_DIST = [0.5, 0.2, 0.1, 0.1, 0.1]\n",
    "IRRELEVANT_PROB_DIST = [0.5, 0.15, 0.15, 0.1, 0.05, 0.05]\n",
    "\n",
    "# Model import\n",
    "\n",
    "LLM_PATH = Path.cwd().joinpath(*MODEL_DICT['model_path'])\n",
    "LLM_PATH.mkdir(parents = True, exist_ok = True)\n",
    "FINETUNED_MODEL_PATH = LLM_PATH.joinpath('CPT', f\"Checkpoint_{MODEL_DICT['name']}\")\n",
    "\n",
    "# Learning rate hyperparameters\n",
    "\n",
    "ATTN_BASE_LR = 5e-5 \n",
    "FFN_BASE_LR = 2e-5\n",
    "NORM_BASE_LR = 5e-5 * 0.1 \n",
    "HEAD_BASE_LR = 5e-5 \n",
    "\n",
    "FFN_GATE_LR = FFN_BASE_LR * 2\n",
    "FFN_BIAS_LR = FFN_BASE_LR * 1\n",
    "\n",
    "ATTN_LORA_A_LR = ATTN_BASE_LR\n",
    "ATTN_LORA_B_LR = ATTN_BASE_LR * 0.5\n",
    "ATTN_GATE_LR = ATTN_BASE_LR * 2\n",
    "ATTN_BIAS_LR = ATTN_BASE_LR * 1\n",
    "\n",
    "# Weight decay hyperparameters\n",
    "\n",
    "WEIGHT_DECAY = 0.01\n",
    "BIAS_DECAY = 0\n",
    "NORM_DECAY = 0\n",
    "LR_LAYER_DECAY = 0.95\n",
    "\n",
    "# Batch size parameters\n",
    "\n",
    "GLOBAL_BATCH_SIZE = 32\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Scheduling and regularization hyperparameters\n",
    "\n",
    "EPOCHS = 20\n",
    "WARMUP = 0.05\n",
    "\n",
    "LAMBDA_MAGNITUDE_DRIFT = 1e-4\n",
    "MAGNITUDE_SHIFT_WARMUP = 0.05\n",
    "\n",
    "# Loop hyper-parameters\n",
    "\n",
    "VALIDATION_LOGGING_FACTOR = 1\n",
    "CHECKPOINT_FREQUENCY_STEPS = 100\n",
    "\n",
    "TOLERANCE = 0\n",
    "PATIENCE = 2\n",
    "\n",
    "GRADIENT_CHECKPOINTING = False\n",
    "\n",
    "# Optional download of config files and parameters\n",
    "\n",
    "if not LLM_PATH.joinpath(MODEL_DICT['model_path'][0]).exists():\n",
    "    snapshot_download(repo_id = MODEL_DICT['repo_id'], allow_patterns = MODEL_DICT['required_files'], \n",
    "                          local_dir = LLM_PATH, use_auth_token = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae5dcc9-bc2e-4ae5-860a-fff29683e8b5",
   "metadata": {},
   "source": [
    "# Fine-tuning Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "1e5ec7c1-a6f2-443a-b0cc-271b68b61e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA adapter class\n",
    "\n",
    "class LoRALinear(nn.Linear):\n",
    "    \n",
    "    def __init__(self, original_module: nn.Linear, rank: int = 16, alpha: float = 16):\n",
    "\n",
    "        # Transfer original attributes\n",
    "        \n",
    "        super().__init__(original_module.in_features, original_module.out_features, bias = original_module.bias is not None,\n",
    "                             device = original_module.weight.device, dtype = original_module.weight.dtype)\n",
    "\n",
    "        self.weight = copy.deepcopy(original_module.weight)\n",
    "        if original_module.bias is not None:\n",
    "            self.bias = copy.deepcopy(original_module.bias)\n",
    "\n",
    "        self.weight.requires_grad = original_module.weight.requires_grad\n",
    "        if original_module.bias is not None:\n",
    "            self.bias.requires_grad = original_module.bias.requires_grad\n",
    "\n",
    "        self.original_weight_norm = self.weight.norm(p = 'fro')\n",
    "\n",
    "        # Add LoRA custom attributes\n",
    "        \n",
    "        self.lora_rank = rank\n",
    "        self.lora_alpha = alpha\n",
    "        self.lora_scale = alpha / rank\n",
    "        self.lora_weight_a = nn.Parameter(torch.zeros(self.out_features, self.lora_rank,\n",
    "            device = self.weight.device, dtype = self.weight.dtype), requires_grad = True)\n",
    "        self.lora_weight_b = nn.Parameter(torch.normal(mean = 0, std = 0.003, size = (self.lora_rank, self.in_features),\n",
    "            device = self.weight.device, dtype = self.weight.dtype), requires_grad = True)\n",
    "        self.lora_gate = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(self.in_features, 1, bias = True, device = self.weight.device, dtype = self.weight.dtype)),\n",
    "            ('act_fn', nn.Sigmoid())\n",
    "        ]))\n",
    "\n",
    "        # Initialize gate \n",
    "\n",
    "        for param in self.lora_gate.parameters():\n",
    "            nn.init.constant_(param, 0.0)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        return nn.functional.linear(input, self.weight + self.lora_gate(input.mean(dim = (0, 1))) \\\n",
    "                                    * self.lora_scale * torch.matmul(self.lora_weight_a, self.lora_weight_b), self.bias)\n",
    "\n",
    "    def magnitude_drift_penalty(self) -> torch.Tensor:\n",
    "        \n",
    "        updated_weight = self.weight + torch.matmul(self.lora_weight_a, self.lora_weight_b)\n",
    "        \n",
    "        return (updated_weight.norm(p = 'fro') - self.original_weight_norm) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def patch_attention(model, rank, alpha, params):\n",
    "        \n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.Linear) and any(key in name.split('.')[-2:] for key in params):\n",
    "                parent = dict(model.named_modules())[name.rsplit('.', 1)[0]]\n",
    "                attr = name.split('.')[-1]\n",
    "                setattr(parent, attr, LoRALinear(module, rank = rank, alpha = alpha))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "38d9bfaa-65a4-451d-8655-f67a0990f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Adapter Class\n",
    "\n",
    "class AdapterLlamaMLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, original_module, adapter_dim):\n",
    "        \n",
    "        # Original MLP attributes\n",
    "        \n",
    "        super().__init__()\n",
    "        self.config = original_module.config\n",
    "        self.hidden_size = original_module.hidden_size\n",
    "        self.intermediate_size = original_module.intermediate_size\n",
    "        \n",
    "        self.gate_proj = copy.deepcopy(original_module.gate_proj)\n",
    "        self.up_proj = copy.deepcopy(original_module.up_proj)\n",
    "        self.down_proj = copy.deepcopy(original_module.down_proj)\n",
    "        self.act_fn = original_module.act_fn\n",
    "\n",
    "        # Added adapter attributes\n",
    "\n",
    "        self.adapter_dim = adapter_dim\n",
    "        self.adapter = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(self.up_proj.in_features, self.adapter_dim, bias = True, \n",
    "                              device = self.up_proj.weight.device, dtype = self.up_proj.weight.dtype)),\n",
    "            ('act_fn', nn.SiLU()),\n",
    "            ('dropout', nn.Dropout(p = 0.1)),\n",
    "            ('fc2', nn.Linear(self.adapter_dim, self.down_proj.out_features, bias = True,\n",
    "                              device = self.down_proj.weight.device, dtype = self.up_proj.weight.dtype))\n",
    "        ]))\n",
    "        self.adapter_gate = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(self.hidden_size, 1, bias = True, device = self.up_proj.weight.device, dtype = self.up_proj.weight.dtype)),\n",
    "            ('act_fn', nn.Sigmoid())\n",
    "        ]))\n",
    "\n",
    "        # Initialize new adapter weights\n",
    "\n",
    "        for param in self.adapter_gate.parameters():\n",
    "            nn.init.constant_(param, 0.0)\n",
    "\n",
    "        for name, param in self.adapter.named_parameters():\n",
    "            if 'fc2' in name.split('.'):\n",
    "                nn.init.normal_(param, mean = 0.0, std = 0.003)\n",
    "            else:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "        adapt_proj = self.adapter(x) * self.adapter_gate(x)\n",
    "        \n",
    "        return down_proj + adapt_proj\n",
    "\n",
    "    # Special function for initiating adapter injection\n",
    "\n",
    "    @staticmethod\n",
    "    def patch_mlp(model, adapter_dim):\n",
    "        \n",
    "        for i in range(len(model.model.layers)):\n",
    "            model.model.layers[i].mlp = AdapterLlamaMLP(\n",
    "                model.model.layers[i].mlp, adapter_dim = adapter_dim)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "id": "205d69a9-7d26-492d-b23f-6a6a9d0edb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a single class for proper model loading\n",
    "\n",
    "class LLMmanager:\n",
    "    \n",
    "    def __init__(self, model_path, device, peft = True, load_8bit = False, finetuned_model_path = None,\n",
    "                     from_pretrained = False, is_dummy = False, dummy_parameters = {}, use_cache = None,\n",
    "                         rope_scaling = None, rope_theta = None, max_context_length = None,\n",
    "                             adapter_dim = None, lora_rank = None, lora_alpha = None, lora_params = None,\n",
    "                                 original_trainable_params = None, trainable_adapter = None, trainable_lora = None):\n",
    "\n",
    "        # Loading parameters \n",
    "        \n",
    "        self.model_path = model_path\n",
    "        self.finetuned_model_path = finetuned_model_path\n",
    "        self.device = torch.device(device)\n",
    "        self.load_8bit = load_8bit\n",
    "        self.from_pretrained = from_pretrained\n",
    "        self.peft = peft\n",
    "        self.use_cache = use_cache\n",
    "\n",
    "        # Dummy model control\n",
    "        \n",
    "        self.is_dummy = is_dummy\n",
    "        self.dummy_parameters = dummy_parameters\n",
    "        self.strict_loading = not self.is_dummy\n",
    "\n",
    "        # Context length parameters\n",
    "\n",
    "        self.rope_scaling = rope_scaling\n",
    "        self.rope_theta = rope_theta\n",
    "        self.max_context_length = max_context_length\n",
    "\n",
    "        # Model files\n",
    "        \n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.gen_config = None\n",
    "        self.checkpoint_report_template = None\n",
    "\n",
    "        # Internal configs\n",
    "\n",
    "        self.config = None\n",
    "        self.finetuning_config = {\n",
    "            'adapter_dim' : 128 if adapter_dim is None else adapter_dim, \n",
    "            'lora_rank' : 16 if lora_rank is None else lora_rank,\n",
    "            'lora_alpha' : 16 if lora_alpha is None else lora_alpha, \n",
    "            'lora_params' : ['q_proj', 'k_proj', 'v_proj', 'o_proj'] if lora_params is None else lora_params,\n",
    "            'original_trainable_params' :  [] if original_trainable_params is None else original_trainable_params,\n",
    "            'trainable_adapter' : True if trainable_adapter is None else trainable_adapter,\n",
    "            'trainable_lora' : True if trainable_lora is None else trainable_lora\n",
    "        }\n",
    "\n",
    "        # Checkpoint parameters\n",
    "        \n",
    "        self.checkpoint_report_filename = 'finetuning_report.txt'\n",
    "        self.finetuning_config_filename = 'finetuning_config.json'\n",
    "\n",
    "    # Main loading function \n",
    "    \n",
    "    def load_model(self, load_finetuned: bool = False, parallelize: bool = False) \\\n",
    "                        -> tuple[PreTrainedTokenizer, PreTrainedModel, GenerationConfig]:\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, local_files_only = True)\n",
    "    \n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "    \n",
    "        self.gen_config = GenerationConfig.from_pretrained(self.model_path)\n",
    "    \n",
    "        if type(self.max_context_length) == int:\n",
    "            setattr(self.gen_config, 'max_length', self.max_context_length)\n",
    "\n",
    "        if 'checkpoint_report_template.txt' in os.listdir(self.model_path):\n",
    "            with open(self.model_path.joinpath('checkpoint_report_template.txt'), 'r') as file:\n",
    "                self.checkpoint_report_template = file.read()\n",
    "    \n",
    "        if self.from_pretrained and not self.is_dummy:\n",
    "        \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_path,\n",
    "                device_map = 'auto' if self.device == torch.device('cuda') else None,\n",
    "                torch_dtype = torch.bfloat16 if self.device == torch.device('cuda') else torch.float32,\n",
    "                load_in_8bit = self.load_8bit,\n",
    "                local_files_only = True,\n",
    "                use_safetensors = True,\n",
    "                rope_scaling = self.rope_scaling\n",
    "            ).to(self.device)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Pulling out the config\n",
    "    \n",
    "            self.config = AutoConfig.from_pretrained(self.model_path, local_files_only = True, use_safetensors = True)\n",
    "            \n",
    "            # Optional config editing for a dummy model\n",
    "            \n",
    "            if self.is_dummy:\n",
    "                for name, value in self.dummy_parameters.items():\n",
    "                    setattr(self.config, name, value)\n",
    "    \n",
    "            if type(self.rope_scaling) == dict:\n",
    "                setattr(self.config, 'rope_scaling', self.rope_scaling)\n",
    "    \n",
    "            if type(self.max_context_length) == int:\n",
    "                setattr(self.config, 'max_position_embeddings', self.max_context_length)\n",
    "    \n",
    "            if self.rope_theta is not None:\n",
    "                setattr(self.config, 'rope_theta', self.rope_theta)\n",
    "\n",
    "            if self.use_cache is not None:\n",
    "                setattr(self.config, 'use_cache', self.use_cache)\n",
    "            \n",
    "            # Instantiate the model\n",
    "            \n",
    "            self.model = AutoModelForCausalLM.from_config(self.config).to(self.device)\n",
    "            \n",
    "            # Load parameters\n",
    "    \n",
    "            if (not load_finetuned) or (self.finetuned_model_path is None):\n",
    "                self.load_safetensors()\n",
    "\n",
    "        # Fine-tuning preparation\n",
    "\n",
    "        if self.peft:\n",
    "            \n",
    "            if load_finetuned and self.finetuned_model_path is not None:\n",
    "                with open(self.finetuned_model_path.joinpath('finetuning_config.json'), 'r') as file:\n",
    "                    self.finetuning_config.update(json.load(file))\n",
    "                    \n",
    "            self.finetune_prepare(load_weights = load_finetuned)\n",
    "\n",
    "        if parallelize and torch.cuda.device_count() > 1 and self.device == torch.device('cuda'):\n",
    "            model = model.to(self.device)\n",
    "            model = nn.DataParallel(model)\n",
    "\n",
    "        return self.tokenizer, self.model, self.gen_config \n",
    "                \n",
    "    # Safetensor loading function \n",
    "    \n",
    "    def load_safetensors(self, model_path = None, model = None):\n",
    "\n",
    "        if model_path is None:\n",
    "            model_path = self.model_path\n",
    "\n",
    "        if model is None:\n",
    "            model = self.model\n",
    "\n",
    "        with open(model_path.joinpath('model.safetensors.index.json'), 'r') as file:\n",
    "            index = json.load(file)\n",
    "        \n",
    "        safetensors_params_map = defaultdict(list)\n",
    "        for param_name, safetensor_name in index['weight_map'].items():\n",
    "            safetensors_params_map[safetensor_name].append(param_name)\n",
    "\n",
    "        full_state_dict = {}\n",
    "        \n",
    "        for safetensor_name, param_names in safetensors_params_map.items():\n",
    "            safetensor_path = model_path.joinpath(safetensor_name)\n",
    "            shard_dict = load_file(safetensor_path)\n",
    "            model_dict = model.state_dict()\n",
    "        \n",
    "            shard_dict = {\n",
    "                key : value for key, value in shard_dict.items()\n",
    "                if key in model_dict and model_dict[key].shape == value.shape\n",
    "            }\n",
    "\n",
    "            full_state_dict.update(shard_dict)\n",
    "        \n",
    "        with warnings.catch_warnings(action = 'ignore'):\n",
    "            model.load_state_dict(full_state_dict, strict = self.strict_loading)\n",
    "\n",
    "    # Function for fine-tuning preparation\n",
    "    \n",
    "    def finetune_prepare(self, model = None, load_weights = False):\n",
    "\n",
    "        if model is None:\n",
    "            model = self.model\n",
    "    \n",
    "        # Freeze original parameters\n",
    "    \n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "        # Inject LoRA modules\n",
    "    \n",
    "        LoRALinear.patch_attention(model, self.finetuning_config['lora_rank'], self.finetuning_config['lora_alpha'],\n",
    "                                       self.finetuning_config['lora_params'])\n",
    "    \n",
    "        # Inject Adapters\n",
    "    \n",
    "        AdapterLlamaMLP.patch_mlp(model, self.finetuning_config['adapter_dim'])\n",
    "    \n",
    "        if load_weights and self.finetuned_model_path is not None:\n",
    "            self.load_safetensors(model_path = self.finetuned_model_path)\n",
    "    \n",
    "        # Unfreeze selected modules\n",
    "\n",
    "        trainable_params = []\n",
    "        trainable_params.extend(self.finetuning_config['original_trainable_params'])\n",
    "        if self.finetuning_config['trainable_adapter']:\n",
    "            trainable_params.extend(self.finetuning_config.get('adapter_param_names',\n",
    "                [name for name, param in model.named_parameters() if 'adapter' in name]))\n",
    "        if self.finetuning_config['trainable_lora']:\n",
    "            trainable_params.extend(self.finetuning_config.get('lora_param_names',\n",
    "                [name for name, param in model.named_parameters() if 'lora' in name]))\n",
    "    \n",
    "        for name, param in model.named_parameters():\n",
    "            if any([pname in name for pname in trainable_params]):\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    # Utility for saving checkpoints\n",
    "\n",
    "    def save_model_checkpoint(self, output_dir, log_param_list: list = None,\n",
    "                                  model: PreTrainedModel = None, tokenizer: PreTrainedTokenizer = None):\n",
    "\n",
    "        if model is None:\n",
    "            model = self.model\n",
    "            \n",
    "        if tokenizer is None:\n",
    "            tokenizer = self.tokenizer\n",
    "        \n",
    "        # Collect model parameters\n",
    "        \n",
    "        adapter_dim = None\n",
    "        lora_rank = None\n",
    "        lora_alpha = None\n",
    "        adapter_param_names = []\n",
    "        lora_param_names = []\n",
    "        \n",
    "        is_adapter = False\n",
    "        is_lora = False\n",
    "        \n",
    "        for mname, module in model.named_modules():\n",
    "                \n",
    "            if isinstance(module, AdapterLlamaMLP):\n",
    "        \n",
    "                if not is_adapter:\n",
    "        \n",
    "                    adapter_dim = module.adapter_dim\n",
    "                    is_adapter = True\n",
    "                \n",
    "                for pname, param in module.named_parameters():\n",
    "                    if any([(keyword in pname.split('.')) for keyword in ['adapter', 'adapter_gate']]):\n",
    "                        adapter_param_names.append('.'.join([mname, pname]))\n",
    "        \n",
    "            if isinstance(module, LoRALinear):\n",
    "        \n",
    "                if not is_lora:\n",
    "        \n",
    "                    lora_rank = module.lora_rank\n",
    "                    lora_alpha = module.lora_alpha\n",
    "                    is_lora = True\n",
    "                \n",
    "                for pname, param in module.named_parameters():\n",
    "                    if any([(keyword in pname.split('.')) for keyword in ['lora_weight_a', 'lora_weight_b', 'lora_gate']]):\n",
    "                        lora_param_names.append('.'.join([mname, pname]))\n",
    "    \n",
    "        lora_params = list(set([name.split('.')[4] for name in lora_param_names]))\n",
    "    \n",
    "        finetuning_config = {\n",
    "            'adapter_dim' : adapter_dim, \n",
    "            'lora_rank' : lora_rank,\n",
    "            'lora_alpha' : lora_alpha, \n",
    "            'lora_params' : lora_params,\n",
    "            'adapter_param_names' : adapter_param_names, \n",
    "            'lora_param_names' : lora_param_names\n",
    "        }\n",
    "            \n",
    "        # Save files\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok = True)\n",
    "        model.save_pretrained(output_dir, safe_serialization = True) \n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "        with open(output_dir.joinpath(self.finetuning_config_filename), 'w') as f:\n",
    "            json.dump(finetuning_config, f, indent = 2)\n",
    "\n",
    "        if log_param_list is not None and self.checkpoint_report_template is not None:\n",
    "            txt_log = self.checkpoint_report_template.format(*log_param_list)\n",
    "            with open(output_dir.joinpath(self.checkpoint_report_filename), 'w') as f:\n",
    "                f.write(txt_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "id": "5e14299a-019a-472c-b96f-46a946a374a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LoRALinear(\n",
       "            in_features=2, out_features=256, bias=False\n",
       "            (lora_gate): Sequential(\n",
       "              (fc1): Linear(in_features=2, out_features=1, bias=True)\n",
       "              (act_fn): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (k_proj): LoRALinear(\n",
       "            in_features=2, out_features=64, bias=False\n",
       "            (lora_gate): Sequential(\n",
       "              (fc1): Linear(in_features=2, out_features=1, bias=True)\n",
       "              (act_fn): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (v_proj): LoRALinear(\n",
       "            in_features=2, out_features=64, bias=False\n",
       "            (lora_gate): Sequential(\n",
       "              (fc1): Linear(in_features=2, out_features=1, bias=True)\n",
       "              (act_fn): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "          (o_proj): LoRALinear(\n",
       "            in_features=256, out_features=2, bias=False\n",
       "            (lora_gate): Sequential(\n",
       "              (fc1): Linear(in_features=256, out_features=1, bias=True)\n",
       "              (act_fn): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mlp): AdapterLlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2, out_features=4, bias=False)\n",
       "          (up_proj): Linear(in_features=2, out_features=4, bias=False)\n",
       "          (down_proj): Linear(in_features=4, out_features=2, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "          (adapter): Sequential(\n",
       "            (fc1): Linear(in_features=2, out_features=128, bias=True)\n",
       "            (act_fn): SiLU()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       "          )\n",
       "          (adapter_gate): Sequential(\n",
       "            (fc1): Linear(in_features=2, out_features=1, bias=True)\n",
       "            (act_fn): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 745,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and  prepare the LLM for fine-tuning\n",
    "\n",
    "llm_utility = \\\n",
    "    LLMmanager(LLM_PATH, DEVICE, finetuned_model_path = FINETUNED_MODEL_PATH,\n",
    "               peft = PEFT, from_pretrained = FROM_PRETRAINED, load_8bit = LOAD_8BIT,\n",
    "               is_dummy = IS_DUMMY, dummy_parameters = DUMMY_PARAMETERS,\n",
    "               rope_scaling = ROPE_SCALING, rope_theta = ROPE_THETA, max_context_length = MAX_CONTEXT_LENGTH,\n",
    "               adapter_dim = ADAPTER_DIM, lora_rank = LORA_RANK, lora_alpha = LORA_ALPHA, lora_params = LORA_PARAMS,\n",
    "               original_trainable_params = ORIGINAL_TRAINABLE_PARAMS,\n",
    "               trainable_adapter = TRAINABLE_ADAPTER, trainable_lora = TRAINABLE_LORA,\n",
    "               use_cache = USE_CACHE)\n",
    "\n",
    "tokenizer, llm_model, gen_config = llm_utility.load_model(load_finetuned = LOAD_FINETUNED, parallelize = PARALLELIZE)\n",
    "\n",
    "llm_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825e2687-a572-45f6-b643-4c52248c4845",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "id": "0b08f764-b8ca-4d68-9e75-93604a07bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a tokenizer\n",
    "\n",
    "with open(LLM_PATH.joinpath('system_prompt.txt')) as sp, \\\n",
    "    open(LLM_PATH.joinpath('generation_template.txt')) as gt:\n",
    "        \n",
    "    system_prompt = sp.read()\n",
    "    generation_template = gt.read()\n",
    "\n",
    "tokenizer.chat_template = generation_template\n",
    "\n",
    "# Declare a special dataset class for CPT (Continued Pre-training) and SFT datasets\n",
    "\n",
    "class FineTuneDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, dataset, dataset_type, target_col = None, system_prompt = None, query_col = None, id_col = None,\n",
    "                     context_dataset = None, query_con_col = None, answer_con_col = None, query_match_col = None, sim_match_col = None,\n",
    "                         num_retrieved = None, includes_gold_prob = None, relevant_prob_dist = None,\n",
    "                             irrelevant_prob_dist = None, train = True, max_context_length = 16000, max_generation_length = 1000):\n",
    "\n",
    "        if dataset_type == 'SFT' and (query_col is None or context_dataset is None \n",
    "                                      or query_match_col is None or sim_match_col is None\n",
    "                                      or query_con_col is None or answer_con_col is None):\n",
    "            raise ValueError('SFT dataset must have valid query, context and embeddings')\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train = train\n",
    "        self.dataset_type = dataset_type\n",
    "        self.max_context_length = max_context_length\n",
    "        self.max_generation_length = max_generation_length\n",
    "\n",
    "        # Context evaluation phrases for sampling\n",
    "\n",
    "        self.relevant_comments = [\n",
    "            'The retrieved cases are relevant and provide actionable insights applicable to this query.',\n",
    "            'Relevant prior cases were found and directly informed the suggested actions.',\n",
    "            'The context from retrieved tickets was useful and guided the resolution strategy.',\n",
    "            'Prior cases align closely with the current issue and offer practical guidance.',\n",
    "            'Useful precedents were identified, supporting the recommended troubleshooting steps.'\n",
    "        ]\n",
    "\n",
    "        self.semi_relevant_comments = [\n",
    "            'The retrieved cases are partially relevant. Some suggestions may apply but require adaptation.',\n",
    "            'Prior tickets contain useful hints but do not fully address the current situation.',\n",
    "            'Context provides limited insights; additional investigation may be necessary.',\n",
    "            'The retrieved cases offer partial guidance, but not all information is directly applicable.',\n",
    "            'Some elements from prior cases can inform the solution, but verification is needed.'\n",
    "        ]\n",
    "\n",
    "        self.non_relevant_comments = [\n",
    "            'The retrieved cases do not appear to be relevant to this query.',\n",
    "            'No prior cases provide actionable insights for the current issue.',\n",
    "            'Context from retrieved tickets is not applicable; alternative strategies should be considered.',\n",
    "            'The prior cases are irrelevant. Guidance must be derived from general troubleshooting principles.',\n",
    "            'Retrieved tickets do not contribute to resolving this query.'\n",
    "        ]\n",
    "\n",
    "        if self.dataset_type == 'CPT':\n",
    "            self.samples = dataset\n",
    "\n",
    "        if self.dataset_type == 'SFT':\n",
    "            \n",
    "            dataset = dataset.merge(context_dataset[[query_match_col, sim_match_col]],\n",
    "                                        how = 'left', left_on = id_col, right_index = True)\n",
    "            \n",
    "            self.system_prompt = system_prompt\n",
    "            self.system_prompt_length = self.tokenizer(self.system_prompt, add_special_tokens = False,\n",
    "                                                           return_tensors = 'pt')['attention_mask'].size()[1]\n",
    "            \n",
    "            self.context_dataset = context_dataset\n",
    "            self.context_len = len(context_dataset)\n",
    "            self.min_context_len = 300\n",
    "            self.query_con_col = query_con_col\n",
    "            self.answer_con_col = answer_con_col\n",
    "            self.num_retrieved = num_retrieved if num_retrieved is not None else 5\n",
    "            self.includes_gold_prob = includes_gold_prob if includes_gold_prob is not None else 0.7\n",
    "            self.relevant_prob_dist = relevant_prob_dist if relevant_prob_dist is not None else [1 / self.num_retrieved] * self.num_retrieved\n",
    "            self.irrelevant_prob_dist = irrelevant_prob_dist if irrelevant_prob_dist is not None else \\\n",
    "                                            [1 / (self.num_retrieved + 1)] * (self.num_retrieved + 1)\n",
    "            self.samples = list((query, context, target) for query, context, target in \\\n",
    "                                    zip(dataset[query_col].tolist(),\n",
    "                                        zip(dataset[query_match_col].tolist(), dataset[sim_match_col].tolist()),\n",
    "                                            dataset[target_col].tolist()))\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Get data item depending on CPT or SFT training\n",
    "\n",
    "        if self.dataset_type == 'CPT':\n",
    "            text = self.tokenizer.bos_token + self.samples[idx] + self.tokenizer.eos_token\n",
    "            whitespace_sep_text = text.split()\n",
    "            instruct_text = whitespace_sep_text[:int(len(whitespace_sep_text) / 2)]\n",
    "            target_text = whitespace_sep_text[int(len(whitespace_sep_text) / 2):]\n",
    "\n",
    "        if self.dataset_type == 'SFT':\n",
    "\n",
    "            # Retrieve context documents and adjust context length based on global context length\n",
    "\n",
    "            retrieved_context, comment = self.retrieve_context(idx)\n",
    "            retrieved_context = [(rank, doc) for rank, doc in sorted(retrieved_context, key = lambda x: x[0], reverse = True)]\n",
    "            answer = comment + '\\n\\n' + self.samples[idx][2].rstrip('<|eot_id|>')\n",
    "            \n",
    "            retrieved_context_tokenized = [\n",
    "                (rank, self.tokenizer(doc, add_special_tokens = False, return_tensors = 'pt')['input_ids'].squeeze(0))\n",
    "                    for rank, doc in retrieved_context\n",
    "            ]\n",
    "\n",
    "            retrieved_context_lengths = torch.tensor([ids.size(0) for rank, ids in retrieved_context_tokenized])\n",
    "\n",
    "            query_length = self.tokenizer(self.samples[idx][0], add_special_tokens = False,\n",
    "                                              return_tensors = 'pt')['attention_mask'].size()[1]\n",
    "            answer_length = self.tokenizer(answer, add_special_tokens = False,\n",
    "                                              return_tensors = 'pt')['attention_mask'].size()[1]\n",
    "\n",
    "            excess_length = (self.max_context_length \n",
    "                                 - self.system_prompt_length \n",
    "                                 - query_length\n",
    "                                 - answer_length\n",
    "                                 - retrieved_context_lengths.sum().item())\n",
    "            \n",
    "            excess_length = -excess_length if excess_length < 0 else 0\n",
    "\n",
    "            if excess_length > 0:\n",
    "                \n",
    "                allowed_context_lengths = torch.clamp(retrieved_context_lengths - \n",
    "                                                        torch.ceil((retrieved_context_lengths / retrieved_context_lengths.sum()) \n",
    "                                                                       * excess_length).to(torch.long), min = 400)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                allowed_context_lengths = retrieved_context_lengths\n",
    "\n",
    "            # Apply chat templates and distingush between instructions and generation parts\n",
    "\n",
    "            retrieved_documents = [\n",
    "                {'content' : self.tokenizer.decode(ids[:allowed_context_lengths[i].item()], skip_special_tokens = True)}\n",
    "                     for i, (rank, ids) in enumerate(retrieved_context_tokenized)\n",
    "            ]\n",
    "            \n",
    "            text = self.tokenizer.apply_chat_template(\n",
    "                conversation = [\n",
    "                    {'role' : 'system', 'content' : self.system_prompt},\n",
    "                    {'role' : 'user', 'content' : self.samples[idx][0]},\n",
    "                    {'role' : 'assistant', 'content' : answer}\n",
    "                ],\n",
    "                documents = retrieved_documents,\n",
    "                add_generation_prompt = False,\n",
    "                tokenize = False\n",
    "            ).strip()\n",
    "\n",
    "            instruct_text = self.tokenizer.apply_chat_template(\n",
    "                conversation = [\n",
    "                    {'role' : 'system', 'content' : self.system_prompt},\n",
    "                    {'role' : 'user', 'content' : self.samples[idx][0]},\n",
    "                    {'role' : 'assistant', 'content' : ''}\n",
    "                ],\n",
    "                documents = retrieved_documents,\n",
    "                add_generation_prompt = False,\n",
    "                tokenize = False\n",
    "            ).strip()\n",
    "\n",
    "            target_text = text[len(instruct_text)-10:]\n",
    "\n",
    "        # Tokenize text/template\n",
    "            \n",
    "        encoding = self.tokenizer(text, return_tensors = 'pt', add_special_tokens = False)\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "\n",
    "        # Get position to start loss calculation\n",
    "\n",
    "        if self.dataset_type == 'CPT':\n",
    "\n",
    "            labels = input_ids.clone()\n",
    "\n",
    "        if self.dataset_type == 'SFT':\n",
    "        \n",
    "            mask_start = self.tokenizer(instruct_text, return_tensors = 'pt', add_special_tokens = False)['input_ids'].squeeze(0).size()[0]\n",
    "\n",
    "            # Mask an input up to expected generated text\n",
    "            \n",
    "            labels = input_ids.clone()\n",
    "            labels[:mask_start-1] = -100\n",
    "\n",
    "        return input_ids, labels, instruct_text, target_text, self.train\n",
    "        \n",
    "    def retrieve_context(self, idx):\n",
    "\n",
    "        includes_gold = np.random.rand() <= self.includes_gold_prob\n",
    "\n",
    "        if includes_gold:\n",
    "\n",
    "            random_num = self.num_retrieved - np.random.choice(list(range(self.num_retrieved)), p = self.relevant_prob_dist)\n",
    "            retrieval_idx = [self.samples[idx][1][0][i] if i < random_num\n",
    "                                 else self.context_dataset.index[np.random.randint(0, self.context_len)]\n",
    "                                     for i in range(self.num_retrieved)]\n",
    "            match_sim = [round(self.samples[idx][1][1][i], 3) if i < random_num \n",
    "                             else round(np.random.rand() * 0.05, 3)\n",
    "                                 for i in range(self.num_retrieved)]\n",
    "            \n",
    "        else:\n",
    "\n",
    "            random_num = self.num_retrieved - np.random.choice(list(range(self.num_retrieved + 1)), p = self.irrelevant_prob_dist)\n",
    "            retrieval_idx = [self.samples[idx][1][0][1:][i] if i < random_num\n",
    "                                 else self.context_dataset.index[np.random.randint(0, self.context_len)]\n",
    "                                     for i in range(self.num_retrieved)]\n",
    "            match_sim = [round(self.samples[idx][1][1][1:][i], 3) if i < random_num\n",
    "                             else round(np.random.rand() * 0.05, 3)\n",
    "                                 for i in range(self.num_retrieved)]\n",
    "            \n",
    "        content = list(zip(match_sim,\n",
    "                           list(map(lambda i: self.context_dataset.loc[i][self.query_con_col] + '\\n\\n' \\\n",
    "                                        + self.context_dataset.loc[i][self.answer_con_col],\n",
    "                                    retrieval_idx))))\n",
    "        \n",
    "        if includes_gold:\n",
    "            comment = np.random.choice(self.relevant_comments)\n",
    "        elif np.array(match_sim).max() >= 0.6:\n",
    "            comment = np.random.choice(self.semi_relevant_comments)\n",
    "        else:\n",
    "            comment = np.random.choice(self.non_relevant_comments)\n",
    "            \n",
    "        return content, comment\n",
    "\n",
    "    def batch_collate(self, batch):\n",
    "        \n",
    "        input_ids, labels, instruct_text, target_text, is_train = zip(*batch)\n",
    "        input_ids = nn.utils.rnn.pad_sequence(input_ids, batch_first = True, padding_value = self.tokenizer.pad_token_id)\n",
    "        labels = nn.utils.rnn.pad_sequence(labels, batch_first = True, padding_value = -100)\n",
    "        attention_mask = (input_ids != self.tokenizer.pad_token_id).to(torch.long)\n",
    "        \n",
    "        return input_ids, attention_mask, labels, instruct_text, target_text, is_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "id": "7376ace4-4853-45ee-b402-dedbc17fc4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a dataset\n",
    "\n",
    "if VALIDATION:\n",
    "\n",
    "    validation_idx = np.random.choice(list(processed_tickets_df['TICKETID']), int(len(processed_tickets_df.index) * 0.1), replace = False)\n",
    "    validation_mask = processed_tickets_df['TICKETID'].isin(validation_idx)\n",
    "\n",
    "    processed_tickets_df[~validation_mask].to_json(data_folder.joinpath('llm_processed_tickets_df_train.json'),\n",
    "                                                       orient = 'index', double_precision = 15, index = True)\n",
    "    processed_tickets_df[validation_mask].to_json(data_folder.joinpath('llm_processed_tickets_df_val.json'),\n",
    "                                                       orient = 'index', double_precision = 15, index = True)\n",
    "\n",
    "    dataset_train = FineTuneDataset(tokenizer, processed_tickets_df[~validation_mask], DATASET_TYPE,\n",
    "                                    target_col = 'SOLUTION', system_prompt = system_prompt, query_col = 'PROBLEM',\n",
    "                                    context_dataset = context_tickets_df, query_con_col = 'Query', id_col = 'TICKETID',\n",
    "                                    answer_con_col = 'Summarized Answer', query_match_col = 'context_ids', sim_match_col = 'context_sims',\n",
    "                                    num_retrieved = NUM_RETRIEVED, includes_gold_prob = INCLUDES_GOLD_PROB,\n",
    "                                    relevant_prob_dist = RELEVANT_PROB_DIST, irrelevant_prob_dist = IRRELEVANT_PROB_DIST,\n",
    "                                    max_context_length = MAX_CONTEXT_LENGTH, max_generation_length = MAX_GENERATION_LENGTH, train = True)\n",
    "\n",
    "    dataset_val = FineTuneDataset(tokenizer, processed_tickets_df[validation_mask], DATASET_TYPE,\n",
    "                                  target_col = 'SOLUTION', system_prompt = system_prompt, query_col = 'PROBLEM',\n",
    "                                  context_dataset = context_tickets_df, query_con_col = 'Query', id_col = 'TICKETID',\n",
    "                                  answer_con_col = 'Summarized Answer', query_match_col = 'context_ids', sim_match_col = 'context_sims',\n",
    "                                  num_retrieved = NUM_RETRIEVED, includes_gold_prob = INCLUDES_GOLD_PROB,\n",
    "                                  relevant_prob_dist = RELEVANT_PROB_DIST, irrelevant_prob_dist = IRRELEVANT_PROB_DIST,\n",
    "                                  max_context_length = MAX_CONTEXT_LENGTH, max_generation_length = MAX_GENERATION_LENGTH, train = False)\n",
    "    \n",
    "else:\n",
    "\n",
    "    dataset_train = FineTuneDataset(tokenizer, processed_tickets_df, DATASET_TYPE,\n",
    "                                    target_col = 'SOLUTION', system_prompt = system_prompt, query_col = 'PROBLEM',\n",
    "                                    context_dataset = context_tickets_df, id_col = 'TICKETID', query_con_col = 'Query',\n",
    "                                    answer_con_col = 'Summarized Answer', query_match_col = 'context_ids', sim_match_col = 'context_sims',\n",
    "                                    num_retrieved = NUM_RETRIEVED, includes_gold_prob = INCLUDES_GOLD_PROB,\n",
    "                                    relevant_prob_dist = RELEVANT_PROB_DIST, irrelevant_prob_dist = IRRELEVANT_PROB_DIST,\n",
    "                                    max_context_length = MAX_CONTEXT_LENGTH, max_generation_length = MAX_GENERATION_LENGTH, train = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee21576-009d-4e7d-9975-9bc983abc237",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d3badca0-f8f5-4047-9bab-77d57ef438b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of parameter-specific learning rates and weight decays\n",
    "\n",
    "def create_parameter_groups(model):\n",
    "\n",
    "    parameter_optim_list = []\n",
    "    num_layers = len(model.model.layers)\n",
    "    \n",
    "    for mname, module in model.named_modules():\n",
    "    \n",
    "        if len(mname.split('.')) >= 3 and 'layers' in mname.split('.'):\n",
    "    \n",
    "            layer_num = int(mname.split('.')[2]) + 1\n",
    "            \n",
    "            if isinstance(module, AdapterLlamaMLP):\n",
    "                for pname, param in module.named_parameters():\n",
    "                    if 'weight' in pname.split('.') and 'adapter' in pname.split('.'):\n",
    "                        parameter_optim_list.append({'params' : param, 'weight_decay': WEIGHT_DECAY,\n",
    "                                                         'lr': FFN_BASE_LR * (LR_LAYER_DECAY ** (num_layers - layer_num))})\n",
    "                    elif 'weight' in pname.split('.') and 'adapter_gate' in pname.split('.'):\n",
    "                        parameter_optim_list.append({'params' : param, 'weight_decay': WEIGHT_DECAY, 'lr': FFN_GATE_LR})\n",
    "                    elif 'bias' in pname.split('.') and 'adapter' in pname.split('.'):\n",
    "                        parameter_optim_list.append({'params' : param, 'weight_decay': BIAS_DECAY,\n",
    "                                                         'lr': FFN_BIAS_LR * (LR_LAYER_DECAY ** (num_layers - layer_num))})\n",
    "                    elif 'bias' in pname.split('.') and 'adapter_gate' in pname.split('.'):\n",
    "                        parameter_optim_list.append({'params' : param, 'weight_decay': BIAS_DECAY, 'lr': FFN_BIAS_LR})\n",
    "        \n",
    "            if isinstance(module, LoRALinear):\n",
    "                for pname, param in module.named_parameters():\n",
    "                    if 'lora_weight_a' in pname.split('.'):\n",
    "                        parameter_optim_list.append({'params' : param, 'weight_decay': WEIGHT_DECAY,\n",
    "                                                         'lr': ATTN_LORA_A_LR * (LR_LAYER_DECAY ** (num_layers - layer_num))})\n",
    "                    elif 'lora_weight_b' in pname.split('.'):\n",
    "                        parameter_optim_list.append({'params' : param, 'weight_decay': WEIGHT_DECAY,\n",
    "                                                         'lr': ATTN_LORA_B_LR * (LR_LAYER_DECAY ** (num_layers - layer_num))})\n",
    "                    elif 'weight' in pname.split('.') and 'lora_gate' in pname.split('.'):\n",
    "                        parameter_optim_list.append({'params' : param, 'weight_decay': WEIGHT_DECAY, 'lr': ATTN_GATE_LR})\n",
    "                    elif 'bias' in pname.split('.') and 'lora_gate' in pname.split('.'):\n",
    "                        parameter_optim_list.append({'params' : param, 'weight_decay': BIAS_DECAY, 'lr': ATTN_BIAS_LR})\n",
    "                        \n",
    "            if isinstance(module, model.model.norm.__class__):\n",
    "                for pname, param in module.named_parameters():\n",
    "                    parameter_optim_list.append({'params' : param, 'weight_decay': NORM_DECAY, \n",
    "                                                 'lr': NORM_BASE_LR * (LR_LAYER_DECAY ** (num_layers - layer_num))})\n",
    "    \n",
    "        if isinstance(module, model.model.norm.__class__) and 'layers' not in mname.split('.'):\n",
    "            for pname, param in module.named_parameters():\n",
    "                parameter_optim_list.append({'params' : param, 'weight_decay': NORM_DECAY, 'lr': NORM_BASE_LR})\n",
    "                    \n",
    "        if 'lm_head' in mname.split('.') and 'layers' not in mname.split('.'):\n",
    "            for pname, param in module.named_parameters():\n",
    "                parameter_optim_list.append({'params' : param, 'weight_decay': WEIGHT_DECAY, 'lr': HEAD_BASE_LR})\n",
    "                \n",
    "    return parameter_optim_list\n",
    "\n",
    "# Learning rate scheduler instantiation\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles = 0.5, min_lambda_lr = 0.1):\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        \n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        \n",
    "        return max(min_lambda_lr , 0.5 * (1.0 + torch.cos(torch.tensor(num_cycles * torch.pi * 2.0 * progress))).item())\n",
    "\n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Define loss function\n",
    "\n",
    "def llm_finetuning_loss_fn(logits, labels, lora_modules, lambda_magnitude_drift,\n",
    "                               vocab_size, ignore_index = -100, label_smoothing = 0):\n",
    "    \n",
    "    cross_entropy = nn.CrossEntropyLoss(ignore_index = -100, reduction = 'mean', label_smoothing = label_smoothing) \\\n",
    "                        (logits.view(-1, vocab_size), labels.view(-1))\n",
    "    magnitude_regularization = torch.stack([module.magnitude_drift_penalty() for module in lora_modules]).mean()\n",
    "    \n",
    "    return cross_entropy + lambda_magnitude_drift * magnitude_regularization\n",
    "\n",
    "# Memory allocation tracking\n",
    "\n",
    "def check_memory(device, size, label = '', reporting_threshold_1 = 0.2, reporting_threshold_2 = 0.7):\n",
    "\n",
    "    available = torch.cuda.get_device_properties(device.index).total_memory / 1024**3\n",
    "    allocated = torch.cuda.memory_allocated(device) / 1024 ** 3\n",
    "    reserved = torch.cuda.memory_reserved(device) / 1024 ** 3\n",
    "    max_alloc = torch.cuda.max_memory_allocated(device) / 1024 ** 3\n",
    "\n",
    "    if (reserved / available) >= reporting_threshold_1:\n",
    "        print(f\"[{label}][rank{device}] allocated={allocated:.2f} GB, reserved={reserved:.2f} GB, peak={max_alloc:.2f} GB, tensor={size}\")\n",
    "\n",
    "    if (reserved / available) >= reporting_threshold_2:\n",
    "\n",
    "        snapshot = torch.cuda.memory_snapshot()\n",
    "        with open('snapshot.json', 'w') as sn, open('memory_summary.txt', 'w') as msu, open('memory_stats.txt', 'w') as mst:\n",
    "            json.dump(snapshot, sn, indent = 2)\n",
    "            msu.write(torch.cuda.memory_summary(device = device, abbreviated = False))\n",
    "            json.dump(torch.cuda.memory_stats(device), mst, indent = 2)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0226c4a4-2701-4ce2-b6f9-5866007752b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tuning Loop\n",
    "\n",
    "def train(rank, world_size):\n",
    "\n",
    "    # Initialize process group (if multi-GPU)\n",
    "\n",
    "    global_rank = int(os.environ['SLURM_PROCID'])\n",
    "\n",
    "    parallelized = False\n",
    "    device = DEVICE\n",
    "\n",
    "    if device == torch.device('cuda'):\n",
    "        \n",
    "        torch.backends.cuda.enable_flash_sdp(True)\n",
    "        torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "        torch.backends.cuda.enable_math_sdp(False)\n",
    "    \n",
    "    if world_size > 1 and device == torch.device('cuda'):\n",
    "\n",
    "        torch.cuda.set_device(rank)\n",
    "        device = torch.device(f'cuda:{rank}')\n",
    "        \n",
    "        dist.init_process_group('nccl', rank = global_rank, world_size = world_size, device_id = device)\n",
    "        parallelized = True\n",
    "\n",
    "    # Initiate dynamically updated parameters\n",
    "\n",
    "    best_score = np.inf\n",
    "    patience_epochs = 0\n",
    "    training_steps = 0\n",
    "    steps = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_perplex = []\n",
    "    val_perplex = []\n",
    "\n",
    "    gradient_accumulation = \\\n",
    "        int(np.ceil(GLOBAL_BATCH_SIZE / (BATCH_SIZE * world_size)))\n",
    "\n",
    "    num_training_steps = int(np.ceil((len(dataset_train) / GLOBAL_BATCH_SIZE) * EPOCHS))\n",
    "    num_warmup_steps = int(np.ceil(num_training_steps * WARMUP))\n",
    "        \n",
    "    # Construct the model\n",
    "\n",
    "    llm_utility = \\\n",
    "        LLMmanager(LLM_PATH, device, finetuned_model_path = FINETUNED_MODEL_PATH,\n",
    "                   peft = PEFT, from_pretrained = FROM_PRETRAINED, load_8bit = LOAD_8BIT,\n",
    "                   is_dummy = IS_DUMMY, dummy_parameters = DUMMY_PARAMETERS,\n",
    "                   rope_scaling = ROPE_SCALING, rope_theta = ROPE_THETA, max_context_length = MAX_CONTEXT_LENGTH,\n",
    "                   adapter_dim = ADAPTER_DIM, lora_rank = LORA_RANK, lora_alpha = LORA_ALPHA, lora_params = LORA_PARAMS,\n",
    "                   original_trainable_params = ORIGINAL_TRAINABLE_PARAMS,\n",
    "                   trainable_adapter = TRAINABLE_ADAPTER, trainable_lora = TRAINABLE_LORA,\n",
    "                   use_cache = USE_CACHE)\n",
    "\n",
    "    tokenizer, model, _ = llm_utility.load_model(load_finetuned = LOAD_FINETUNED)\n",
    "\n",
    "    # Broadcast parameters from a source process\n",
    "    \n",
    "    if parallelized:\n",
    "        \n",
    "        dist.barrier()\n",
    "        \n",
    "        for param in model.parameters():\n",
    "            dist.broadcast(param.data, src = 0)\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "    # Set up dataloaders\n",
    "\n",
    "    if parallelized:\n",
    "        sampler_train = DistributedSampler(dataset_train, num_replicas = world_size, rank = global_rank, shuffle = True)\n",
    "    else:\n",
    "        sampler_train = None\n",
    "\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size = BATCH_SIZE, sampler = sampler_train, \n",
    "                                      shuffle = (sampler_train is None), collate_fn = dataset_train.batch_collate)\n",
    "\n",
    "    if VALIDATION:\n",
    "\n",
    "        if parallelized:\n",
    "            sampler_val = DistributedSampler(dataset_val, num_replicas = world_size, rank = global_rank, shuffle = False)\n",
    "        else:\n",
    "            sampler_val = None\n",
    "            \n",
    "        dataloader_val = DataLoader(dataset_val, batch_size = BATCH_SIZE, sampler = sampler_val, \n",
    "                                        shuffle = (sampler_val is None), collate_fn = dataset_val.batch_collate)\n",
    "\n",
    "    # Memory optimizations\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    if GRADIENT_CHECKPOINTING:\n",
    "        model.gradient_checkpointing_enable()\n",
    "        if hasattr(model, 'enable_input_require_grads'):\n",
    "            model.enable_input_require_grads()\n",
    "\n",
    "    # Wrap the model into DDP class and define optimizer and schedule\n",
    "\n",
    "    if parallelized:\n",
    "        model = DDP(model, device_ids = [rank], output_device = rank, gradient_as_bucket_view = True)\n",
    "        model_module = model.module\n",
    "    else:\n",
    "        model_module = model\n",
    "\n",
    "    optimizer = optim.AdamW(create_parameter_groups(model_module))\n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "                    optimizer, \n",
    "                    num_warmup_steps = num_warmup_steps, \n",
    "                    num_training_steps = num_training_steps\n",
    "    )\n",
    "\n",
    "    lora_modules = [module for name, module in model_module.named_modules() if isinstance(module, LoRALinear)]\n",
    "\n",
    "    # Training loop\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "    \n",
    "        # Training\n",
    "\n",
    "        if sampler_train is not None:\n",
    "            sampler_train.set_epoch(epoch)\n",
    "    \n",
    "        epoch_train_loss = []\n",
    "        model.train()\n",
    "\n",
    "        accumulated_loss = torch.zeros(1, device = device) \n",
    "        \n",
    "        for input_ids, attention_mask, labels, _, _, _ in tqdm(dataloader_train):\n",
    "            \n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "    \n",
    "            logits = model(input_ids = input_ids, attention_mask = attention_mask).logits\n",
    "            shifted_logits = logits[:, :-1, :].contiguous()\n",
    "            shifted_labels = labels[:, 1:].contiguous()\n",
    "    \n",
    "            magnitude_schedule = min((training_steps + 1) / (MAGNITUDE_SHIFT_WARMUP * num_training_steps), 1)\n",
    "            \n",
    "            loss = llm_finetuning_loss_fn(shifted_logits, shifted_labels, lora_modules, magnitude_schedule * LAMBDA_MAGNITUDE_DRIFT,\n",
    "                                             model_module.model.embed_tokens.num_embeddings, ignore_index = -100, label_smoothing = 0.1) \\\n",
    "                                                 / gradient_accumulation\n",
    "\n",
    "            # Print memory usage statistics\n",
    "            \n",
    "            if device.type == 'cuda':\n",
    "                check_memory(device, input_ids.size(), label = f'forward step {steps}')\n",
    "\n",
    "            # Loss accumulation\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                accumulated_loss += loss.detach()\n",
    "\n",
    "            # Optimization steps\n",
    "    \n",
    "            steps += 1\n",
    "            if steps % gradient_accumulation == 0:\n",
    "                \n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    if parallelized:\n",
    "                        dist.all_reduce(accumulated_loss, op = dist.ReduceOp.SUM)\n",
    "                        loss_avg = (accumulated_loss / dist.get_world_size()).item()\n",
    "                    else:\n",
    "                        loss_avg = accumulated_loss.item()\n",
    "                \n",
    "                epoch_train_loss.append(loss_avg)\n",
    "                accumulated_loss = torch.zeros(1, device = device) \n",
    "                training_steps += 1\n",
    "\n",
    "            # Intermediate checkpointing\n",
    "    \n",
    "            if training_steps % CHECKPOINT_FREQUENCY_STEPS == 0 and global_rank == 0:\n",
    "    \n",
    "                checkpoint_source = 'Ongoing intermediate checkpointing'\n",
    "                log_list = \\\n",
    "                    [str(datetime.datetime.now()), dataset_train.dataset_type, training_steps, epoch + 1,\n",
    "                     str(train_losses), str(val_losses), str(train_perplex), str(val_perplex), checkpoint_source, str(epoch_train_loss),\n",
    "                     ATTN_BASE_LR, FFN_BASE_LR, NORM_BASE_LR, HEAD_BASE_LR, WEIGHT_DECAY, BIAS_DECAY, NORM_DECAY,\n",
    "                     LR_LAYER_DECAY, WARMUP, EPOCHS, GLOBAL_BATCH_SIZE, LAMBDA_MAGNITUDE_DRIFT, MAGNITUDE_SHIFT_WARMUP]\n",
    "                llm_utility.save_model_checkpoint(FINETUNED_MODEL_PATH, log_param_list = log_list, \n",
    "                                                      model = model_module, tokenizer = tokenizer)\n",
    "    \n",
    "        train_losses.append(torch.tensor(epoch_train_loss).mean().item())\n",
    "        train_perplex.append(torch.exp(torch.tensor(epoch_train_loss).mean()).item())\n",
    "        if global_rank == 0:\n",
    "            print(f'Epoch {epoch + 1} train loss: {torch.tensor(epoch_train_loss).mean().item():.4f}')\n",
    "    \n",
    "        # Validation\n",
    "    \n",
    "        if (epoch + 1) % VALIDATION_LOGGING_FACTOR == 0 and VALIDATION:\n",
    "\n",
    "            if sampler_val is not None:\n",
    "                sampler_val.set_epoch(epoch)\n",
    "                        \n",
    "            epoch_val_loss = []\n",
    "            model.eval()\n",
    "    \n",
    "            with torch.no_grad():\n",
    "            \n",
    "                for input_ids, attention_mask, labels, _, _, _ in dataloader_val:\n",
    "                    \n",
    "                    input_ids = input_ids.to(device)\n",
    "                    attention_mask = attention_mask.to(device)\n",
    "                    labels = labels.to(device)\n",
    "            \n",
    "                    logits = model(input_ids = input_ids, attention_mask = attention_mask).logits\n",
    "                    shifted_logits = logits[:, :-1, :].contiguous()\n",
    "                    shifted_labels = labels[:, 1:].contiguous()\n",
    "                    \n",
    "                    loss = llm_finetuning_loss_fn(shifted_logits, shifted_labels, lora_modules, 0,\n",
    "                                                    model_module.model.embed_tokens.num_embeddings,\n",
    "                                                        ignore_index = -100, label_smoothing = 0.1)\n",
    "\n",
    "                    loss_tensor = loss.detach()\n",
    "                    if parallelized:\n",
    "                        dist.all_reduce(loss_tensor, op = dist.ReduceOp.SUM)\n",
    "                        loss_avg = (loss_tensor / dist.get_world_size()).item()\n",
    "                    else:\n",
    "                        loss_avg = loss_tensor.item()\n",
    "                    epoch_val_loss.append(loss_avg)\n",
    "\n",
    "                val_losses.append(torch.tensor(epoch_val_loss).mean().item())\n",
    "                val_perplex.append(torch.exp(torch.tensor(epoch_val_loss).mean()).item())\n",
    "                if global_rank == 0:\n",
    "                    print(f'Epoch {epoch + 1} validation loss: {torch.tensor(epoch_val_loss).mean().item():.4f}')\n",
    "    \n",
    "        else:\n",
    "            \n",
    "            val_losses.append(np.nan)\n",
    "            val_perplex.append(np.nan)\n",
    "    \n",
    "        # Early stopping\n",
    "    \n",
    "        if VALIDATION and global_rank == 0:\n",
    "\n",
    "            # Post-validation checkpoint\n",
    "    \n",
    "            if not np.isnan(val_losses[-1]) and val_losses[-1] < best_score * (1 + TOLERANCE):\n",
    "                best_score = val_losses[-1]\n",
    "    \n",
    "                checkpoint_source = 'Post validation improvement checkpointing'\n",
    "                log_list = \\\n",
    "                    [str(datetime.datetime.now()), dataset_train.dataset_type, training_steps, epoch + 1,\n",
    "                     str(train_losses), str(val_losses), str(train_perplex), str(val_perplex), checkpoint_source, str(epoch_train_loss),\n",
    "                     ATTN_BASE_LR, FFN_BASE_LR, NORM_BASE_LR, HEAD_BASE_LR, WEIGHT_DECAY, BIAS_DECAY, NORM_DECAY,\n",
    "                     LR_LAYER_DECAY, WARMUP, EPOCHS, GLOBAL_BATCH_SIZE, LAMBDA_MAGNITUDE_DRIFT, MAGNITUDE_SHIFT_WARMUP]\n",
    "                llm_utility.save_model_checkpoint(FINETUNED_MODEL_PATH, log_param_list = log_list, \n",
    "                                                      model = model_module, tokenizer = tokenizer)\n",
    "                patience_epochs = 0\n",
    "                \n",
    "            elif not np.isnan(val_losses[-1]) and val_losses[-1] >= best_score * (1 + TOLERANCE):\n",
    "                patience_epochs += 1\n",
    "                if patience_epochs >= PATIENCE:\n",
    "                    print('Early stopping triggered!')\n",
    "                    break\n",
    "\n",
    "        # Final checkpoint\n",
    "        \n",
    "        elif (epoch + 1) == EPOCHS and global_rank == 0:\n",
    "    \n",
    "            checkpoint_source = 'Final training checkpoint'\n",
    "            log_list = \\\n",
    "                [str(datetime.datetime.now()), dataset_train.dataset_type, training_steps, epoch + 1,\n",
    "                 str(train_losses), str(val_losses), str(train_perplex), str(val_perplex), checkpoint_source, str(epoch_train_loss),\n",
    "                 ATTN_BASE_LR, FFN_BASE_LR, NORM_BASE_LR, HEAD_BASE_LR, WEIGHT_DECAY, BIAS_DECAY, NORM_DECAY,\n",
    "                 LR_LAYER_DECAY, WARMUP, EPOCHS, GLOBAL_BATCH_SIZE, LAMBDA_MAGNITUDE_DRIFT, MAGNITUDE_SHIFT_WARMUP]\n",
    "            llm_utility.save_model_checkpoint(FINETUNED_MODEL_PATH, log_param_list = log_list, \n",
    "                                                  model = model_module, tokenizer = tokenizer)\n",
    "\n",
    "    # Clean up processes\n",
    "    \n",
    "    if parallelized:\n",
    "        dist.destroy_process_group()\n",
    "        \n",
    "def main():\n",
    "    \n",
    "    world_size = torch.cuda.device_count()\n",
    "\n",
    "    if world_size > 1 and DEVICE == torch.device('cuda'):\n",
    "        # Spawn processes, one per GPU\n",
    "        mp.spawn(train, args = (world_size, ), nprocs = world_size, join = True)\n",
    "        \n",
    "    else:  \n",
    "        # Single process training\n",
    "        train(rank = 0, world_size = 1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8fb814-8b66-49e7-9fdf-947727f0fd77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

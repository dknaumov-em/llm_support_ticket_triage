
//Fine-tuning checkpoint//

Datetime: 2025-08-22 15:27:37.211166
Last training step: 1630
Last epoch: 10
Epoch train losses: [2.932002305984497, 1.56087064743042, 1.1323609352111816, 0.9139717221260071, 0.750351071357727, 0.6270235776901245, 0.5221412777900696, 0.4273456037044525, 0.36296164989471436, 0.30014896392822266]
Epoch validation losses: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
Checkpoint source: Final training checkpoint

Last batch infoNCE losses: [0.31552839279174805, 0.3641986846923828, 0.1974998414516449, 0.3133520185947418, 0.47771155834198, 0.36387744545936584, 0.20734693109989166, 0.3828895688056946, 0.25090891122817993, 0.3288300633430481, 0.21718807518482208, 0.3151865005493164, 0.25406980514526367, 0.3038312792778015, 0.2726709246635437, 0.31891143321990967, 0.4501011371612549, 0.35606127977371216, 0.21329723298549652, 0.31270819902420044, 0.3746395409107208, 0.2330370992422104, 0.3577212691307068, 0.30811288952827454, 0.3640037477016449, 0.23962709307670593, 0.33017462491989136, 0.30614352226257324, 0.2658959627151489, 0.40000393986701965, 0.22388505935668945, 0.4253688454627991, 0.24229846894741058, 0.25986289978027344, 0.19130513072013855, 0.41881975531578064, 0.4745078980922699, 0.19919121265411377, 0.2293776422739029, 0.20444776117801666, 0.4034062623977661, 0.20249296724796295, 0.2636343836784363, 0.39609915018081665, 0.27320557832717896, 0.276685506105423, 0.16650788486003876, 0.3859824538230896, 0.25380009412765503, 0.3618766665458679, 0.2293984442949295, 0.2795579731464386, 0.2537163496017456, 0.2424030900001526, 0.34204959869384766, 0.4496881365776062, 0.18085098266601562, 0.3876218795776367, 0.18087591230869293, 0.21866656839847565, 0.46756964921951294, 0.3172651529312134, 0.3042464852333069, 0.21901708841323853, 0.3608749806880951, 0.24590635299682617, 0.3143618106842041, 0.2798163890838623, 0.40048912167549133, 0.19695936143398285, 0.39519554376602173, 0.5937706828117371, 0.38241857290267944, 0.3571036159992218, 0.22783580422401428, 0.39742571115493774, 0.39561229944229126, 0.21453623473644257, 0.2114894986152649, 0.25663667917251587, 0.259105920791626, 0.3132254481315613, 0.4316490590572357, 0.22217360138893127, 0.3084169626235962, 0.3032636046409607, 0.40087515115737915, 0.33623552322387695, 0.4783470034599304, 0.2418830841779709, 0.16919490694999695, 0.3228835463523865, 0.20223082602024078, 0.2143518030643463, 0.42027878761291504, 0.2713266909122467, 0.27959325909614563, 0.21929894387722015, 0.18679220974445343, 0.19742633402347565, 0.30044853687286377, 0.2394910305738449, 0.38889575004577637, 0.3645986020565033, 0.18420952558517456, 0.361139178276062, 0.241120383143425, 0.30811721086502075, 0.2663475275039673, 0.3701271414756775, 0.3163905143737793, 0.21547125279903412, 0.17049328982830048, 0.2956809103488922, 0.22453705966472626, 0.3115301728248596, 0.32324257493019104, 0.32643136382102966, 0.25988322496414185, 0.2999938130378723, 0.22665569186210632, 0.3605893850326538, 0.5335575342178345, 0.3263288140296936, 0.4258452355861664, 0.26275673508644104, 0.17996662855148315, 0.2559860348701477, 0.2353447675704956, 0.3713127076625824, 0.22252294421195984, 0.2459479719400406, 0.41508251428604126, 0.24806208908557892, 0.25184035301208496, 0.2212747186422348, 0.27975669503211975, 0.3321188986301422, 0.22122518718242645, 0.27230894565582275, 0.2872435450553894, 0.37925899028778076, 0.25641196966171265, 0.3917219936847687, 0.35209304094314575, 0.18756556510925293, 0.3043099045753479, 0.24362842738628387, 0.24678367376327515, 0.22481273114681244, 0.3572957515716553, 0.34550991654396057, 0.3177618384361267, 0.3140503168106079, 0.28968459367752075, 0.3483310341835022, 0.24741515517234802, 0.2387567013502121, 0.3805164694786072, 0.2244901955127716, 0.38314205408096313, 0.3663007616996765, 0.25036823749542236]

//Hyperparameters//

Query embedding base learning rate: 1e-05
Query encoder base learning rate: 2e-05
Query pooler base learning rate: 5e-05

Answer embedding base learning rate: 2e-05
Answer encoder base learning rate: 5e-05
Answer pooler base learning rate: 0.0001

Similarity loss temperature: 0.07
Weight decay: 0.01
Layer learning rate decay: 0.95
LR Warmup: 0.05
LR Sheduling: One cycle cosine annealing (Pooler) + Squared growth annealing (Embedding and Encoder)
Max number of epochs: 10
Batch size: 32

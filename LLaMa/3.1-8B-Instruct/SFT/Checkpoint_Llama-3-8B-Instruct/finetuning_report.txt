//Fine-tuning checkpoint//

Datetime: 2025-09-03 16:45:35.509557
Training type: SFT
Last training step: 1462
Last epoch: 10
Epoch train losses: [1.7416431903839111, 1.3888683319091797, 1.3541526794433594, 1.3360114097595215, 1.3322895765304565, 1.3234879970550537, 1.3152490854263306, 1.3132215738296509, 1.3205708265304565, 1.3165258169174194]
Epoch validation losses: [1.3641526699066162, 1.3216947317123413, 1.3024940490722656, 1.2927284240722656, 1.2776142358779907, 1.2811298370361328, 1.2773737907409668, 1.2750600576400757, 1.2753605842590332, 1.2743089199066162]
Epoch train perplexities: [5.70671272277832, 4.010309219360352, 3.8734774589538574, 3.8038413524627686, 3.789710283279419, 3.7565011978149414, 3.7256789207458496, 3.718132734298706, 3.745558738708496, 3.730438709259033]
Epoch validation perplexities: [3.9124064445495605, 3.7497708797454834, 3.678459405899048, 3.642711877822876, 3.588069200515747, 3.600705623626709, 3.5872066020965576, 3.578916311264038, 3.5799920558929443, 3.5762290954589844]
Checkpoint source: Post validation improvement checkpointing

Last batch CE losses: [1.0244140625, 1.288330078125, 1.30224609375, 1.280029296875, 1.33056640625, 1.353515625, 1.33984375, 1.371337890625, 1.3505859375, 1.292236328125, 1.315185546875, 1.341064453125, 1.28173828125, 1.296875, 1.307373046875, 1.34326171875, 1.3223876953125, 1.31787109375, 1.374267578125, 1.329345703125, 1.3359375, 1.341796875, 1.31494140625, 1.322998046875, 1.35205078125, 1.330322265625, 1.309814453125, 1.294921875, 1.343505859375, 1.338623046875, 1.343505859375, 1.3134765625, 1.322998046875, 1.33251953125, 1.30029296875, 1.314453125, 1.3427734375, 1.32275390625, 1.3017578125, 1.292724609375, 1.337646484375, 1.3505859375, 1.3115234375, 1.337890625, 1.304931640625, 1.3603515625, 1.28857421875, 1.336181640625, 1.31591796875, 1.324462890625, 1.28759765625, 1.30615234375, 1.328857421875, 1.30322265625, 1.316162109375, 1.297119140625, 1.305419921875, 1.362548828125, 1.323974609375, 1.32421875, 1.3155517578125, 1.309814453125, 1.299560546875, 1.28662109375, 1.331298828125, 1.290771484375, 1.331787109375, 1.314208984375, 1.31787109375, 1.283935546875, 1.270751953125, 1.343505859375, 1.301513671875, 1.3125, 1.300048828125, 1.3076171875, 1.289306640625, 1.338134765625, 1.323486328125, 1.34716796875, 1.27783203125, 1.280029296875, 1.2978515625, 1.3115234375, 1.348388671875, 1.340087890625, 1.306640625, 1.315673828125, 1.2919921875, 1.325439453125, 1.359130859375, 1.2886962890625, 1.3095703125, 1.324462890625, 1.3154296875, 1.35888671875, 1.333251953125, 1.34326171875, 1.28369140625, 1.302978515625, 1.34375, 1.343505859375, 1.30712890625, 1.3076171875, 1.3623046875, 1.26025390625, 1.36767578125, 1.29443359375, 1.296630859375, 1.348388671875, 1.28662109375, 1.280029296875, 1.287109375, 1.34765625, 1.295166015625, 1.312744140625, 1.313720703125, 1.2958984375, 1.30859375, 1.298583984375, 1.32763671875, 1.31787109375, 1.322265625, 1.303466796875, 1.32177734375, 1.29638671875, 1.343017578125, 1.322265625, 1.33251953125, 1.297119140625, 1.26953125, 1.30517578125, 1.340576171875, 1.265625, 1.319580078125, 1.301025390625, 1.3369140625, 1.32421875, 1.3291015625, 1.37109375, 1.417236328125, 1.31005859375, 1.299072265625, 1.342529296875, 1.325439453125, 1.3251953125]

//Hyperparameters//

Attention base learning rate: 0.0002
Feed-forward base learning rate: 6e-05
Norm base learning rate: 5e-06
Head base learning rate: 2e-05
Weight decay: 0.01
Bias decay: 0
Norm decay: 0
Layer learning rate decay: 0.95
LR Warmup: 0.05
LR Scheduling: One cycle cosine annealing
Max number of epochs: 10
Batch size: 32

Magnitude drift lambda: 0.0001
Magnitude drift warmup: 0.1
//Fine-tuning checkpoint//

Datetime: 2025-08-28 16:06:48.770053
Training type: CPT
Last training step: 242
Last epoch: 20
Epoch train losses: [2.5592448711395264, 2.0367839336395264, 1.8393758535385132, 1.6720784902572632, 1.5170389413833618, 1.3526865243911743, 1.1522674560546875, 0.9510932564735413, 0.8342514038085938, 0.6614678502082825, 0.511566162109375, 0.4025859832763672, 0.33664003014564514, 0.2860310971736908, 0.23608779907226562, 0.22107256948947906, 0.22082042694091797, 0.21123187243938446, 0.20298290252685547, 0.19536463916301727]
Epoch validation losses: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
Epoch train perplexities: [12.926053047180176, 7.665915489196777, 6.292609691619873, 5.323220729827881, 4.558706760406494, 3.867802619934082, 3.1653621196746826, 2.58853816986084, 2.3030893802642822, 1.9376343488693237, 1.6679013967514038, 1.495687484741211, 1.4002349376678467, 1.3311338424682617, 1.2662855386734009, 1.247413992881775, 1.24709951877594, 1.235198736190796, 1.2250515222549438, 1.2157542705535889]
Epoch validation perplexities: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
Checkpoint source: Final training checkpoint

Last batch CE losses: [0.0977630615234375, 0.14350128173828125, 0.21425247192382812, 0.23664093017578125, 0.12929916381835938, 0.22274017333984375, 0.1408233642578125, 0.2725486755371094, 0.2555046081542969, 0.2474822998046875, 0.18759918212890625, 0.19622039794921875]

//Hyperparameters//

Attention base learning rate: 5e-05
Feed-forward base learning rate: 0.0005
Norm base learning rate: 5e-06
Head base learning rate: 5e-05
Weight decay: 0.01
Bias decay: 0
Norm decay: 0
Layer learning rate decay: 0.95
LR Warmup: 0.05
LR Scheduling: One cycle cosine annealing
Max number of epochs: 20
Batch size: 32

Magnitude drift lambda: 0.0001
Magnitude drift warmup: 0.05